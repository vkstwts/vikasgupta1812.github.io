<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Data Analysis and Machine Learning]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>http://localhost:2368/</link><generator>Ghost 0.5</generator><lastBuildDate>Sun, 07 Dec 2014 21:16:04 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Vizualization using GoogleViz in R]]></title><description><![CDATA[<p>Install and load the package  </p>

<pre><code class="language-r">#install.packages("googleVis", dependencies=TRUE)
library(googleVis)  
</code></pre>

<p>Take Population data  </p>

<pre><code class="language-r">#mapping 
head(Population[,-5]) ##Example data  
</code></pre>

<pre><code>##   Rank       Country Population % of World Population Mode       Date
## 1    1         China 1339940000                0.1950 TRUE 2010-10-09
## 2    2         India 1188650000                0.1730 TRUE 2010-10-09
## 3    3 United States  310438000                0.0452 TRUE 2010-10-09
## 4    4     Indonesia  237556363                0.0346 TRUE 2010-10-09
## 5    5        Brazil  193626000                0.0282 TRUE 2010-10-09
## 6    6      Pakistan  170745000                0.0248 TRUE 2010-10-09
</code></pre>

<p>Size of data  </p>

<pre><code class="language-r">dim(Population)  
</code></pre>

<pre><code>## [1] 195   7
</code></pre>

<p>Generate Geo Map from Gviz  </p>

<pre><code class="language-r">Geo=gvisGeoMap(Population, locationvar="Country",  
               numvar="Population", options=list(height=350, dataMode='regions'))
</code></pre>

<p>Plot the map.  </p>

<pre><code class="language-r">plot(Geo)  
</code></pre>

<iframe src="https://38dce00e545674752c0d41d06778d8ab3b786087.googledrive.com/host/0B0J1O2jMMERWRS1CNi1pclEtNWM/plot1.html" width="556px" height="350px" scrolling="no" marginwidth="0" marginheight="0" frameborder="0">  
</iframe>

<h2 id="example2">Example # 2  </h2>

<p>Get age data</p>

<pre><code class="language-r">input=read.csv("http://www.rci.rutgers.edu/~rwomack/Rexamples/UNdata.csv",header=TRUE)  
Map&lt;- data.frame(input$Country.or.Area, input$Value)  
names(Map)&lt;- c("Country", "Age")  
head(Map)  
</code></pre>

<pre><code>##       Country    Age
## 1 Afghanistan 58.677
## 2 Afghanistan 58.677
## 3 Afghanistan 58.677
## 4 Afghanistan 58.677
## 5 Afghanistan 56.715
## 6 Afghanistan 56.715
</code></pre>

<p>Size of data (10k Rows)  </p>

<pre><code class="language-r">dim(Map)  
</code></pre>

<pre><code>## [1] 10076     2
</code></pre>

<p>Generate Geo Map from Gviz  </p>

<pre><code class="language-r">Geo=gvisGeoMap(Map, locationvar="Country", numvar="Age",  
               options=list(height=350, dataMode='regions'))
</code></pre>

<p>Plot the map  </p>

<pre><code class="language-r">plot(Geo)  
</code></pre>

<iframe src="https://38dce00e545674752c0d41d06778d8ab3b786087.googledrive.com/host/0B0J1O2jMMERWRS1CNi1pclEtNWM/plot2.html" width="556px" height="350px" scrolling="no" marginwidth="0" marginheight="0" frameborder="0">  
</iframe>

<h2 id="motioncharts">Motion charts  </h2>

<p>Get Fruits data  </p>

<pre><code class="language-r">Fruits ## Example data  
</code></pre>

<pre><code>##     Fruit Year Location Sales Expenses Profit       Date
## 1  Apples 2008     West    98       78     20 2008-12-31
## 2  Apples 2009     West   111       79     32 2009-12-31
## 3  Apples 2010     West    89       76     13 2010-12-31
## 4 Oranges 2008     East    96       81     15 2008-12-31
## 5 Bananas 2008     East    85       76      9 2008-12-31
## 6 Oranges 2009     East    93       80     13 2009-12-31
## 7 Bananas 2009     East    94       78     16 2009-12-31
## 8 Oranges 2010     East    98       91      7 2010-12-31
## 9 Bananas 2010     East    81       71     10 2010-12-31
</code></pre>

<p>Small dataset  </p>

<pre><code class="language-r">dim(Fruits)  
</code></pre>

<pre><code>## [1] 9 7
</code></pre>

<p>Generate Motion chart with Gviz  </p>

<pre><code class="language-r">M &lt;- gvisMotionChart(Fruits, idvar="Fruit", timevar="Year")  
</code></pre>

<p>Plot the motion map  </p>

<pre><code class="language-r">plot(M)  
</code></pre>

<iframe src="https://38dce00e545674752c0d41d06778d8ab3b786087.googledrive.com/host/0B0J1O2jMMERWRS1CNi1pclEtNWM/plot3.html" width="600px" height="500px" scrolling="no" marginwidth="0" marginheight="0" frameborder="0">  
</iframe>]]></description><link>http://localhost:2368/googleviz/</link><guid isPermaLink="false">2be59701-e6be-4baa-856c-91f0461c272c</guid><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Mon, 24 Nov 2014 02:36:19 GMT</pubDate></item><item><title><![CDATA[Topic Modelling]]></title><description><![CDATA[<p>Topicmodeling: </p>

<h2 id="abstractsofjsspapers">Abstracts of JSS papers  </h2>

<p>Get Data</p>

<pre><code class="language-r">#install.packages("corpus.JSS.papers", repos = "http://datacube.wu.ac.at/", type = "source")
data("JSS_papers", package = "corpus.JSS.papers")  
</code></pre>

<p>For reproducibility of results we use only abstracts published up to 2010-08-05 and omit those containing non-ASCII characters in the abstracts.</p>

<pre><code class="language-r">JSS_papers &lt;- JSS_papers[JSS_papers[,"date"] &lt; "2010-08-05",];dim(JSS_papers)  
</code></pre>

<pre><code>## [1] 361  15
</code></pre>

<pre><code class="language-r">JSS_papers &lt;- JSS_papers[sapply(JSS_papers[, "description"],Encoding) == "unknown",];dim(JSS_papers)  
</code></pre>

<pre><code>## [1] 348  15
</code></pre>

<pre><code class="language-r">dim(JSS_papers)  
</code></pre>

<pre><code>## [1] 348  15
</code></pre>

<pre><code class="language-r">prod(dim(JSS_papers))  
</code></pre>

<pre><code>## [1] 5220
</code></pre>

<p>The final data set contains 348 documents. Before analysis we transform it to a <code>Corpus</code> using package tm. <br>
HTML markup in the abstracts for greek letters, subscripting, etc., is removed using package XML</p>

<pre><code class="language-r">library("tm")  
library("XML")  
library("ggplot2")  
</code></pre>

<p>Create function to remove HTML Markup</p>

<pre><code class="language-r">remove_HTML_markup &lt;- function(s) tryCatch({  
 doc &lt;- htmlTreeParse(paste("&lt;!DOCTYPE html&gt;", s), asText = TRUE, trim = FALSE)
 xmlValue(xmlRoot(doc))
 }, error = function(s) s)
</code></pre>

<pre><code class="language-r">#JSS_papers[, "description"][1]
#sapply(JSS_papers[, "description"][1],remove_HTML_markup)


#VectorSource(sapply(JSS_papers[, "description"][1],remove_HTML_markup))


#c  &lt;- Corpus(VectorSource(sapply(JSS_papers[, "description"][1],remove_HTML_markup)));c
</code></pre>

<pre><code class="language-r">corpus &lt;- Corpus(VectorSource(sapply(JSS_papers[, "description"],remove_HTML_markup)));corpus  
</code></pre>

<pre><code>## &lt;&lt;VCorpus (documents: 348, metadata (corpus/indexed): 0/0)&gt;&gt;
</code></pre>

<pre><code class="language-r">attributes(corpus)  
</code></pre>

<pre><code>## $names
## [1] "content" "meta"    "dmeta"  
## 
## $class
## [1] "VCorpus" "Corpus"
</code></pre>

<pre><code class="language-r">class(corpus)  
</code></pre>

<pre><code>## [1] "VCorpus" "Corpus"
</code></pre>

<pre><code class="language-r">mode(corpus)  
</code></pre>

<pre><code>## [1] "list"
</code></pre>

<p>The corpus is exported to a document-term matrix using function <code>DocumentTermMatrix()</code> from package <code>tm</code>. The terms are <code>stemmed</code> and the <code>stop words</code>, <code>punctuation</code>, <code>numbers</code> and <code>terms of length less than 3</code> are removed using the <code>control</code> argument. (We use a C locale for reproducibility.)</p>

<p>Some details about Locale <br>
locale describes aspects of the internationalization of a program. Initially most aspects of the locale of R are set to "C" (which is the default for the C language and reflects North-American usage). R sets <code>LC_CTYPE</code> and <code>LC_COLLATE</code>, which allows the use of a different character set and alphabetic comparisons in that character set (including the use of sort), <code>LC_MONETARY</code> (for use by Sys.localeconv) and <code>LC_TIME</code> may affect the behaviour of <code>as.POSIXlt</code> and <code>strptime</code> and functions which use them (but not <code>date</code>).</p>

<pre><code class="language-r">Sys.setlocale("LC_COLLATE", "C")  
</code></pre>

<pre><code>## [1] "C"
</code></pre>

<p>Document Term Matrix : Requires SnowballC library. Install if it is not already installed</p>

<pre><code class="language-r">JSS_dtm &lt;- DocumentTermMatrix(corpus, control = list(stemming = FALSE,  
                                                     stopwords = TRUE, 
                                                     minWordLength = 3,
                                                     removeNumbers = TRUE, 
                                                     removePunctuation = TRUE));JSS_dtm
</code></pre>

<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 348, terms: 5022)&gt;&gt;
## Non-/sparse entries: 20034/1727622
## Sparsity           : 99%
## Maximal term length: 36
## Weighting          : term frequency (tf)
</code></pre>

<p>Structure of Document Term Matrix.</p>

<pre><code class="language-r">str(JSS_dtm)  
</code></pre>

<pre><code>## List of 6
##  $ i       : int [1:20034] 1 1 1 1 1 1 1 1 1 1 ...
##  $ j       : int [1:20034] 139 289 565 842 902 1073 1189 1215 1236 1577 ...
##  $ v       : num [1:20034] 1 1 1 1 1 3 1 2 1 1 ...
##  $ nrow    : int 348
##  $ ncol    : int 5022
##  $ dimnames:List of 2
##   ..$ Docs : chr [1:348] "1" "2" "3" "4" ...
##   ..$ Terms: chr [1:5022] "abc" "abilities" "ability" "able" ...
##  - attr(*, "class")= chr [1:2] "DocumentTermMatrix" "simple_triplet_matrix"
##  - attr(*, "weighting")= chr [1:2] "term frequency" "tf"
</code></pre>

<p>Variables in the dtm</p>

<pre><code class="language-r">names(JSS_dtm)  
</code></pre>

<pre><code>## [1] "i"        "j"        "v"        "nrow"     "ncol"     "dimnames"
</code></pre>

<p>Class</p>

<pre><code class="language-r">class(JSS_dtm)  
</code></pre>

<pre><code>## [1] "DocumentTermMatrix"    "simple_triplet_matrix"
</code></pre>

<p>Mode</p>

<pre><code class="language-r">mode(JSS_dtm)  
</code></pre>

<pre><code>## [1] "list"
</code></pre>

<p>Document term Martix</p>

<pre><code class="language-r">JSS_dtm # all documents &amp; All terms  
</code></pre>

<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 348, terms: 5022)&gt;&gt;
## Non-/sparse entries: 20034/1727622
## Sparsity           : 99%
## Maximal term length: 36
## Weighting          : term frequency (tf)
</code></pre>

<pre><code class="language-r">JSS_dtm[1,] # 1st Document and all terms  
</code></pre>

<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 1, terms: 5022)&gt;&gt;
## Non-/sparse entries: 39/4983
## Sparsity           : 99%
## Maximal term length: 36
## Weighting          : term frequency (tf)
</code></pre>

<pre><code class="language-r">JSS_dtm[,1] # All documents and First term  
</code></pre>

<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 348, terms: 1)&gt;&gt;
## Non-/sparse entries: 1/347
## Sparsity           : 100%
## Maximal term length: 3
## Weighting          : term frequency (tf)
</code></pre>

<pre><code class="language-r">JSS_dtm$nrow; # Documents  
</code></pre>

<pre><code>## [1] 348
</code></pre>

<pre><code class="language-r">JSS_dtm$ncol # Terms  
</code></pre>

<pre><code>## [1] 5022
</code></pre>

<p>Document</p>

<pre><code class="language-r">length(JSS_dtm$i)  
</code></pre>

<pre><code>## [1] 20034
</code></pre>

<pre><code class="language-r">range(JSS_dtm$i)  
</code></pre>

<pre><code>## [1]   1 348
</code></pre>

<p>Term</p>

<pre><code class="language-r">length(JSS_dtm$j)  
</code></pre>

<pre><code>## [1] 20034
</code></pre>

<pre><code class="language-r">range(JSS_dtm$j)  
</code></pre>

<pre><code>## [1]    1 5022
</code></pre>

<p>Frequency</p>

<pre><code class="language-r">length(JSS_dtm$v)  
</code></pre>

<pre><code>## [1] 20034
</code></pre>

<pre><code class="language-r">range(JSS_dtm$v)  
</code></pre>

<pre><code>## [1]  1 11
</code></pre>

<p>Number of terms whose frequency is 0</p>

<pre><code class="language-r">sum(JSS_dtm$v == 0)  
</code></pre>

<pre><code>## [1] 0
</code></pre>

<p>Number of terms whose frequency is 1</p>

<pre><code class="language-r">sum(JSS_dtm$v == 1)  
</code></pre>

<pre><code>## [1] 16465
</code></pre>

<p>Number of terms with frequency 13</p>

<pre><code class="language-r">sum(JSS_dtm$v == 13)  
</code></pre>

<pre><code>## [1] 0
</code></pre>

<p>Number of terms with frequency 14 </p>

<pre><code class="language-r">sum(JSS_dtm$v == 14)  
</code></pre>

<pre><code>## [1] 0
</code></pre>

<p>plot of frequencies</p>

<pre><code class="language-r">qplot(JSS_dtm$v)  
</code></pre>

<pre><code>## stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-30-1.png" alt="plot of chunk unnamed-chunk-30"></p>

<p><code>mean term frequency-inverse document frequency</code> (<code>tf-idf</code>) over documents containing this term is used to select the vocabulary. This measure allows to omit terms which have low frequency as well as those occurring in many documents. We only include terms which have a tf-idf value of at least 0.1 which is a bit more than the median and ensures that the very frequent terms are omitted.</p>

<p><code>slam</code>: Sparse Lightweight Arrays and Matrices</p>

<p>Data structures and algorithms for sparse arrays and matrices, based on index arrays and simple triplet representations, respectively.</p>

<p>Load Slam package</p>

<pre><code class="language-r">library("slam")  
</code></pre>

<p><code>col_sums</code>: Form row and column sums and means for sparse arrays (currently <code>simple_triplet_matrix</code> only).</p>

<pre><code class="language-r">#?col_sums
</code></pre>

<p>rows is the documents <br>
columns are the terms </p>

<p>Gives terms and their frequencies</p>

<pre><code class="language-r">JSS_dtm$ncol  
</code></pre>

<pre><code>## [1] 5022
</code></pre>

<pre><code class="language-r">length(col_sums(JSS_dtm))  
</code></pre>

<pre><code>## [1] 5022
</code></pre>

<pre><code class="language-r">length(JSS_dtm$j)  
</code></pre>

<pre><code>## [1] 20034
</code></pre>

<pre><code class="language-r">length(JSS_dtm$v)  
</code></pre>

<pre><code>## [1] 20034
</code></pre>

<pre><code class="language-r">JSS_dtm$nrow  
</code></pre>

<pre><code>## [1] 348
</code></pre>

<pre><code class="language-r">JSS_dtm  
</code></pre>

<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 348, terms: 5022)&gt;&gt;
## Non-/sparse entries: 20034/1727622
## Sparsity           : 99%
## Maximal term length: 36
## Weighting          : term frequency (tf)
</code></pre>

<pre><code class="language-r">head(col_sums(JSS_dtm))  
</code></pre>

<pre><code>##       abc abilities   ability      able   absence  absolute 
##         1         1         9        10         1         5
</code></pre>

<pre><code class="language-r">str(col_sums(JSS_dtm))  
</code></pre>

<pre><code>##  Named num [1:5022] 1 1 9 10 1 5 2 1 3 2 ...
##  - attr(*, "names")= chr [1:5022] "abc" "abilities" "ability" "able" ...
</code></pre>

<pre><code class="language-r">class(col_sums(JSS_dtm))  
</code></pre>

<pre><code>## [1] "numeric"
</code></pre>

<p>Distribution of term frequencies - Skewed</p>

<pre><code class="language-r">summary(col_sums(JSS_dtm))  
</code></pre>

<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.000   1.000   2.000   5.084   4.000 389.000
</code></pre>

<pre><code class="language-r">qplot(col_sums(JSS_dtm))  
</code></pre>

<pre><code>## stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-43-1.png" alt="plot of chunk unnamed-chunk-43"></p>

<p>The mean term frequency-inverse document frequency (tf-idf) over documents containing this term is used to select the vocabulary. This measure allows to omit terms which have low frequency as well as those occurring in many documents. We only include terms which have a tf-idf value of at least 0.1 which is a bit more than the median and ensures that the very frequent terms are omitted.</p>

<pre><code class="language-r">term_tfidf &lt;-  
 tapply(JSS_dtm$v/row_sums(JSS_dtm)[JSS_dtm$i], 
        JSS_dtm$j, mean) * log2(nDocs(JSS_dtm)/col_sums(JSS_dtm &gt; 0))
</code></pre>

<pre><code class="language-r">str(term_tfidf)  
</code></pre>

<pre><code>##  num [1:5022(1d)] 0.1624 0.097 0.0641 0.0791 0.0545 ...
##  - attr(*, "dimnames")=List of 1
##   ..$ : chr [1:5022] "1" "2" "3" "4" ...
</code></pre>

<pre><code class="language-r">length(term_tfidf)  
</code></pre>

<pre><code>## [1] 5022
</code></pre>

<pre><code class="language-r">class(term_tfidf)  
</code></pre>

<pre><code>## [1] "array"
</code></pre>

<pre><code class="language-r">#attributes(term_tfidf)
</code></pre>

<pre><code class="language-r">head(term_tfidf)  
</code></pre>

<pre><code>##          1          2          3          4          5          6 
## 0.16236430 0.09704533 0.06413909 0.07907440 0.05447060 0.07900995
</code></pre>

<p>Distribution - Slightly less skewed</p>

<pre><code class="language-r">summary(term_tfidf)  
</code></pre>

<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.02285 0.07746 0.09933 0.12220 0.13620 1.16500
</code></pre>

<pre><code class="language-r">qplot(term_tfidf)  
</code></pre>

<pre><code>## stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-50-1.png" alt="plot of chunk unnamed-chunk-50"></p>

<pre><code class="language-r">dim(JSS_dtm)  
</code></pre>

<pre><code>## [1]  348 5022
</code></pre>

<pre><code class="language-r">length(term_tfidf)  
</code></pre>

<pre><code>## [1] 5022
</code></pre>

<pre><code class="language-r">sum(term_tfidf &gt;= 0.1)  
</code></pre>

<pre><code>## [1] 2497
</code></pre>

<pre><code class="language-r">JSS_dtm &lt;- JSS_dtm[,term_tfidf &gt;= 0.1];  
JSS_dtm$nrow  
</code></pre>

<pre><code>## [1] 348
</code></pre>

<pre><code class="language-r">JSS_dtm &lt;- JSS_dtm[row_sums(JSS_dtm) &gt; 0,]  
JSS_dtm$nrow  
</code></pre>

<pre><code>## [1] 348
</code></pre>

<pre><code class="language-r">summary(col_sums(JSS_dtm))  
</code></pre>

<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.000   1.000   1.000   2.687   3.000  35.000
</code></pre>

<pre><code class="language-r">JSS_dtm  
</code></pre>

<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 348, terms: 2497)&gt;&gt;
## Non-/sparse entries: 4648/864308
## Sparsity           : 99%
## Maximal term length: 36
## Weighting          : term frequency (tf)
</code></pre>

<pre><code class="language-r">summary(col_sums(JSS_dtm))  # 1 - 47  
</code></pre>

<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.000   1.000   1.000   2.687   3.000  35.000
</code></pre>

<pre><code class="language-r">qplot(col_sums(JSS_dtm))  
</code></pre>

<pre><code>## stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-55-1.png" alt="plot of chunk unnamed-chunk-55"></p>

<pre><code class="language-r">library("topicmodels")  
k &lt;- 30  
SEED &lt;- 2010  
jss_TM &lt;- list(  
  VEM = LDA(JSS_dtm, k = k, control = list(seed = SEED)),

  VEM_fixed = LDA(JSS_dtm, k = k, 
                  control = list(estimate.alpha = FALSE, seed = SEED)),

  Gibbs = LDA(JSS_dtm, k = k, method = "Gibbs",
              control = list(seed = SEED, burnin = 1000,
                             thin = 100, iter = 1000)),

  CTM = CTM(JSS_dtm, k = k,
            control = list(seed = SEED,
                           var = list(tol = 10^-4), 
                           em = list(tol = 10^-3)))
  )
</code></pre>

<pre><code class="language-r">str(jss_TM$VEM)  
</code></pre>

<pre><code>## Formal class 'LDA_VEM' [package "topicmodels"] with 14 slots
##   ..@ alpha          : num 0.0076
##   ..@ call           : language LDA(x = JSS_dtm, k = k, control = list(seed = SEED))
##   ..@ Dim            : int [1:2] 348 2497
##   ..@ control        :Formal class 'LDA_VEMcontrol' [package "topicmodels"] with 13 slots
##   .. .. ..@ estimate.alpha: logi TRUE
##   .. .. ..@ alpha         : num 1.67
##   .. .. ..@ seed          : int 2010
##   .. .. ..@ verbose       : int 0
##   .. .. ..@ prefix        : chr "/var/folders/zv/8qg2z4351cj76wbbjspsr0h80000gn/T//RtmppVdTzD/file79d089743c8"
##   .. .. ..@ save          : int 0
##   .. .. ..@ nstart        : int 1
##   .. .. ..@ best          : logi TRUE
##   .. .. ..@ keep          : int 0
##   .. .. ..@ estimate.beta : logi TRUE
##   .. .. ..@ var           :Formal class 'OPTcontrol' [package "topicmodels"] with 2 slots
##   .. .. .. .. ..@ iter.max: int 500
##   .. .. .. .. ..@ tol     : num 1e-06
##   .. .. ..@ em            :Formal class 'OPTcontrol' [package "topicmodels"] with 2 slots
##   .. .. .. .. ..@ iter.max: int 1000
##   .. .. .. .. ..@ tol     : num 1e-04
##   .. .. ..@ initialize    : chr "random"
##   ..@ k              : int 30
##   ..@ terms          : chr [1:2497] "abc" "absorbance" "abundance" "abundant" ...
##   ..@ documents      : chr [1:348] "1" "2" "3" "4" ...
##   ..@ beta           : num [1:30, 1:2497] -232 -362 -232 -232 -362 ...
##   ..@ gamma          : num [1:348, 1:30] 0.000441 0.000677 0.000499 0.000534 0.000101 ...
##   ..@ wordassignments:List of 5
##   .. ..$ i   : int [1:4648] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ j   : int [1:4648] 427 453 614 802 1102 1321 1544 1561 1629 1727 ...
##   .. ..$ v   : num [1:4648] 7 7 7 7 7 7 7 7 7 7 ...
##   .. ..$ nrow: int 348
##   .. ..$ ncol: int 2497
##   .. ..- attr(*, "class")= chr "simple_triplet_matrix"
##   ..@ loglikelihood  : num [1:348] -82.6 -61.6 -102.1 -58.8 -276.9 ...
##   ..@ iter           : int 75
##   ..@ logLiks        : num(0) 
##   ..@ n              : int 6709
</code></pre>

<pre><code class="language-r">jss_TM$VEM@alpha  
</code></pre>

<pre><code>## [1] 0.007597997
</code></pre>

<pre><code class="language-r">str(jss_TM$VEM_fixed)  
</code></pre>

<pre><code>## Formal class 'LDA_VEM' [package "topicmodels"] with 14 slots
##   ..@ alpha          : num 1.67
##   ..@ call           : language LDA(x = JSS_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED))
##   ..@ Dim            : int [1:2] 348 2497
##   ..@ control        :Formal class 'LDA_VEMcontrol' [package "topicmodels"] with 13 slots
##   .. .. ..@ estimate.alpha: logi FALSE
##   .. .. ..@ alpha         : num 1.67
##   .. .. ..@ seed          : int 2010
##   .. .. ..@ verbose       : int 0
##   .. .. ..@ prefix        : chr "/var/folders/zv/8qg2z4351cj76wbbjspsr0h80000gn/T//RtmppVdTzD/file79d0613689b"
##   .. .. ..@ save          : int 0
##   .. .. ..@ nstart        : int 1
##   .. .. ..@ best          : logi TRUE
##   .. .. ..@ keep          : int 0
##   .. .. ..@ estimate.beta : logi TRUE
##   .. .. ..@ var           :Formal class 'OPTcontrol' [package "topicmodels"] with 2 slots
##   .. .. .. .. ..@ iter.max: int 500
##   .. .. .. .. ..@ tol     : num 1e-06
##   .. .. ..@ em            :Formal class 'OPTcontrol' [package "topicmodels"] with 2 slots
##   .. .. .. .. ..@ iter.max: int 1000
##   .. .. .. .. ..@ tol     : num 1e-04
##   .. .. ..@ initialize    : chr "random"
##   ..@ k              : int 30
##   ..@ terms          : chr [1:2497] "abc" "absorbance" "abundance" "abundant" ...
##   ..@ documents      : chr [1:348] "1" "2" "3" "4" ...
##   ..@ beta           : num [1:30, 1:2497] -35.2 -37.6 -36.9 -37.6 -42.8 ...
##   ..@ gamma          : num [1:348, 1:30] 0.0249 0.0277 0.0266 0.0264 0.0134 ...
##   ..@ wordassignments:List of 5
##   .. ..$ i   : int [1:4648] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ j   : int [1:4648] 427 453 614 802 1102 1321 1544 1561 1629 1727 ...
##   .. ..$ v   : num [1:4648] 7 7 7 7 7 7 7 7 7 7 ...
##   .. ..$ nrow: int 348
##   .. ..$ ncol: int 2497
##   .. ..- attr(*, "class")= chr "simple_triplet_matrix"
##   ..@ loglikelihood  : num [1:348] -109.9 -77.5 -104.7 -83.3 -343.8 ...
##   ..@ iter           : int 20
##   ..@ logLiks        : num(0) 
##   ..@ n              : int 6709
</code></pre>

<pre><code class="language-r">str(jss_TM$Gibbs)  
</code></pre>

<pre><code>## Formal class 'LDA_Gibbs' [package "topicmodels"] with 16 slots
##   ..@ seedwords      : NULL
##   ..@ z              : int [1:6709] 1 2 30 30 5 20 5 8 28 6 ...
##   ..@ alpha          : num 1.67
##   ..@ call           : language LDA(x = JSS_dtm, k = k, method = "Gibbs", control = list(seed = SEED,      burnin = 1000, thin = 100, iter = 1000))
##   ..@ Dim            : int [1:2] 348 2497
##   ..@ control        :Formal class 'LDA_Gibbscontrol' [package "topicmodels"] with 14 slots
##   .. .. ..@ delta        : num 0.1
##   .. .. ..@ iter         : int 100
##   .. .. ..@ thin         : int 100
##   .. .. ..@ burnin       : int 1000
##   .. .. ..@ initialize   : chr "random"
##   .. .. ..@ alpha        : num 1.67
##   .. .. ..@ seed         : int 2010
##   .. .. ..@ verbose      : int 0
##   .. .. ..@ prefix       : chr "/var/folders/zv/8qg2z4351cj76wbbjspsr0h80000gn/T//RtmppVdTzD/file79d04057303d"
##   .. .. ..@ save         : int 0
##   .. .. ..@ nstart       : int 1
##   .. .. ..@ best         : logi TRUE
##   .. .. ..@ keep         : int 0
##   .. .. ..@ estimate.beta: logi TRUE
##   ..@ k              : int 30
##   ..@ terms          : chr [1:2497] "abc" "absorbance" "abundance" "abundant" ...
##   ..@ documents      : chr [1:348] "1" "2" "3" "4" ...
##   ..@ beta           : num [1:30, 1:2497] -8.4 -8.42 -8.53 -8.57 -8.48 ...
##   ..@ gamma          : num [1:348, 1:30] 0.0398 0.0437 0.041 0.026 0.0133 ...
##   ..@ wordassignments:List of 5
##   .. ..$ i   : int [1:4648] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ j   : int [1:4648] 427 453 614 802 1102 1321 1544 1561 1629 1727 ...
##   .. ..$ v   : num [1:4648] 1 2 30 5 20 5 8 28 6 23 ...
##   .. ..$ nrow: int 348
##   .. ..$ ncol: int 2497
##   .. ..- attr(*, "class")= chr "simple_triplet_matrix"
##   ..@ loglikelihood  : num -45930
##   ..@ iter           : int 100
##   ..@ logLiks        : num(0) 
##   ..@ n              : int 6709
</code></pre>

<pre><code class="language-r">str(jss_TM$CTM)  
</code></pre>

<pre><code>## Formal class 'CTM_VEM' [package "topicmodels"] with 16 slots
##   ..@ nusquared      : num [1:348, 1:29] 3.87 3.98 3.34 4.15 3.42 ...
##   ..@ mu             : num [1:29] -0.224 -0.311 -0.925 -0.106 -0.66 ...
##   ..@ Sigma          : num [1:29, 1:29] 5.4069 -0.0603 -0.0741 -0.0615 -0.0602 ...
##   ..@ call           : language CTM(x = JSS_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4),      em = list(tol = 10^-3)))
##   ..@ Dim            : int [1:2] 348 2497
##   ..@ control        :Formal class 'CTM_VEMcontrol' [package "topicmodels"] with 12 slots
##   .. .. ..@ cg           :Formal class 'OPTcontrol' [package "topicmodels"] with 2 slots
##   .. .. .. .. ..@ iter.max: int 500
##   .. .. .. .. ..@ tol     : num 1e-05
##   .. .. ..@ seed         : int 2010
##   .. .. ..@ verbose      : int 0
##   .. .. ..@ prefix       : chr "/var/folders/zv/8qg2z4351cj76wbbjspsr0h80000gn/T//RtmppVdTzD/file79d07e76d0d9"
##   .. .. ..@ save         : int 0
##   .. .. ..@ nstart       : int 1
##   .. .. ..@ best         : logi TRUE
##   .. .. ..@ keep         : int 0
##   .. .. ..@ estimate.beta: logi TRUE
##   .. .. ..@ var          :Formal class 'OPTcontrol' [package "topicmodels"] with 2 slots
##   .. .. .. .. ..@ iter.max: int 500
##   .. .. .. .. ..@ tol     : num 1e-04
##   .. .. ..@ em           :Formal class 'OPTcontrol' [package "topicmodels"] with 2 slots
##   .. .. .. .. ..@ iter.max: int 1000
##   .. .. .. .. ..@ tol     : num 0.001
##   .. .. ..@ initialize   : chr "rand"
##   ..@ k              : int 30
##   ..@ terms          : chr [1:2497] "abc" "absorbance" "abundance" "abundant" ...
##   ..@ documents      : chr [1:348] "1" "2" "3" "4" ...
##   ..@ beta           : num [1:30, 1:2497] -73.9 -73.6 -90.8 -72.9 -81 ...
##   ..@ gamma          : num [1:348, 1:30] 0.00061 0.000843 0.001718 0.00047 0.000244 ...
##   ..@ wordassignments:List of 5
##   .. ..$ i   : int [1:4648] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..$ j   : int [1:4648] 427 453 614 802 1102 1321 1544 1561 1629 1727 ...
##   .. ..$ v   : num [1:4648] 25 25 25 25 25 25 25 25 25 25 ...
##   .. ..$ nrow: int 348
##   .. ..$ ncol: int 2497
##   .. ..- attr(*, "class")= chr "simple_triplet_matrix"
##   ..@ loglikelihood  : num [1:348] -91.6 -65 -100.2 -67.9 -305.9 ...
##   ..@ iter           : int 19
##   ..@ logLiks        : num(0) 
##   ..@ n              : int 6709
</code></pre>

<pre><code class="language-r">jss_TM$VEM@alpha  
</code></pre>

<pre><code>## [1] 0.007597997
</code></pre>

<pre><code class="language-r">jss_TM$VEM_fixed@alpha  
</code></pre>

<pre><code>## [1] 1.666667
</code></pre>

<pre><code class="language-r">terms(jss_TM$VEM)  
</code></pre>

<pre><code>##          Topic 1          Topic 2          Topic 3          Topic 4 
##          "bayes"           "item"        "ordinal"        "genetic" 
##          Topic 5          Topic 6          Topic 7          Topic 8 
##       "formulae"     "imputation"   "zeroinflated"    "conditional" 
##          Topic 9         Topic 10         Topic 11         Topic 12 
##           "tree"        "control"         "robust"     "estimators" 
##         Topic 13         Topic 14         Topic 15         Topic 16 
##          "polyk"           "socr"            "roc"          "power" 
##         Topic 17         Topic 18         Topic 19         Topic 20 
##    "conditional"            "www"        "wavelet"         "fields" 
##         Topic 21         Topic 22         Topic 23         Topic 24 
##     "confidence"        "network"        "winbugs" "autoregressive" 
##         Topic 25         Topic 26         Topic 27         Topic 28 
##        "density"   "longitudinal"     "clustering"         "choice" 
##         Topic 29         Topic 30 
##     "generators"      "smoothing"
</code></pre>

<pre><code class="language-r">terms(jss_TM$VEM_fixed)  
</code></pre>

<pre><code>##        Topic 1        Topic 2        Topic 3        Topic 4        Topic 5 
##     "patterns"         "item"      "ordinal"     "matrices"     "formulae" 
##        Topic 6        Topic 7        Topic 8        Topic 9       Topic 10 
##   "imputation"       "graphs"       "kernel"         "dose"      "control" 
##       Topic 11       Topic 12       Topic 13       Topic 14       Topic 15 
##       "robust"   "estimators"     "original"    "xlispstat"        "curve" 
##       Topic 16       Topic 17       Topic 18       Topic 19       Topic 20 
##      "samples"  "conditional"          "www"      "wavelet"       "fields" 
##       Topic 21       Topic 22       Topic 23       Topic 24       Topic 25 
##   "confidence"      "network"      "winbugs"       "vector"      "density" 
##       Topic 26       Topic 27       Topic 28       Topic 29       Topic 30 
## "longitudinal"   "clustering"       "choice"   "generators"    "smoothing"
</code></pre>

<pre><code class="language-r">terms(jss_TM$Gibbs)  
</code></pre>

<pre><code>##        Topic 1        Topic 2        Topic 3        Topic 4        Topic 5 
##      "wavelet"       "vector"         "item"      "density"   "population" 
##        Topic 6        Topic 7        Topic 8        Topic 9       Topic 10 
##    "procedure"        "graph"      "winbugs" "mathematical"          "gui" 
##       Topic 11       Topic 12       Topic 13       Topic 14       Topic 15 
##      "network"   "clustering"       "graphs"         "file"       "gamlss" 
##       Topic 16       Topic 17       Topic 18       Topic 19       Topic 20 
##        "power"     "lispstat"      "control"       "robust"      "designs" 
##       Topic 21       Topic 22       Topic 23       Topic 24       Topic 25 
##       "choice"         "risk"         "java"     "matrices"  "theoretical" 
##       Topic 26       Topic 27       Topic 28       Topic 29       Topic 30 
##   "confidence"        "trees"         "socr"      "copulas"   "estimators"
</code></pre>

<pre><code class="language-r">terms(jss_TM$CTM)  
</code></pre>

<pre><code>##        Topic 1        Topic 2        Topic 3        Topic 4        Topic 5 
##       "robust"     "matrices"          "www"       "kernel"   "imputation" 
##        Topic 6        Topic 7        Topic 8        Topic 9       Topic 10 
##    "criterion"        "polyk"        "vista"   "clustering" "zeroinflated" 
##       Topic 11       Topic 12       Topic 13       Topic 14       Topic 15 
##     "lispstat"         "text"         "file"       "volume"         "socr" 
##       Topic 16       Topic 17       Topic 18       Topic 19       Topic 20 
##      "control"      "winbugs"        "hydra"   "generators"       "graphs" 
##       Topic 21       Topic 22       Topic 23       Topic 24       Topic 25 
##      "designs"   "confidence"   "projection"      "network"   "microarray" 
##       Topic 26       Topic 27       Topic 28       Topic 29       Topic 30 
##   "population"      "wavelet"         "item"   "estimators" "mathematical"
</code></pre>

<pre><code class="language-r">topics(jss_TM$VEM)  
</code></pre>

<pre><code>##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 
##   7   2  28  27  18   1  18  23  21  12  17   7  15   8  24  19  15  18 
##  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 
##  11  25  16   7  22   1   1  16  12  21  25  30   5  18   5   9  30  25 
##  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 
##  14  19  21  28  24  29  22  17  16   3  25   4  13   7  13  28  21  19 
##  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 
##  17  15   8  26   6  10  18  14  20  28  15  28  16  19  23   2  24   3 
##  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90 
##  30   6   1   1   1  18   4  10  16   8  11  11  11   8   2   5  28  20 
##  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 
##  28   5   9   8  12  14  25  15  29  24  13  28  18   6  16  19  25  26 
## 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 
##  13  22  28  19  25   9   3  20  28  25   7  30   8   6   6  27  25  22 
## 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 
##  17  10  20  11  13  22  23  12  16  28  16   8  19  10   2   5   4   7 
## 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 
##  12  16   8  13  12   8   5   2   6  14   8  18  22  17   8  26   5  29 
## 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 
##  11  22  27  27  29   1  28   7   6  19   6  10   8   2  10   5  21   2 
## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 
##  20  14   5   1  17  14  16  21  13  16  26   9   2  23  26   6   2   3 
## 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 
##  26  29   4  19  23  16   3  10   9  12  19  23  30  22  14  23  16  15 
## 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 
##  20  29  15   9  10   7  21  23  25   6  20   8  14   8  27  15  27   8 
## 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 
##  20  30   2   8  30  22  12  11  22  10  22  27  27   3  22   7  22  13 
## 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 
##   4   5   5   5   1  12   2  11  24  24  12  29  30   7  14  24  11   9 
## 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 
##  20  22  11  17   9   2  14   4  25  25  14   2   3  15  16  30  24  23 
## 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 
##  26  14   9  16  30  30  15  13   3  23  22  20  20  26  29   3  30   8 
## 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 
##  19  30   9  28  30   1  15  25  11  23   6  28  18  22   7  27  26  29 
## 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 
##  19   4  13  20   2  10   9   3  23  28  11  30  26  12  17  27   3  13 
## 343 344 345 346 347 348 
##  24  20  26  12   7   2
</code></pre>

<pre><code class="language-r">terms(jss_TM$VEM_fixed)  
</code></pre>

<pre><code>##        Topic 1        Topic 2        Topic 3        Topic 4        Topic 5 
##     "patterns"         "item"      "ordinal"     "matrices"     "formulae" 
##        Topic 6        Topic 7        Topic 8        Topic 9       Topic 10 
##   "imputation"       "graphs"       "kernel"         "dose"      "control" 
##       Topic 11       Topic 12       Topic 13       Topic 14       Topic 15 
##       "robust"   "estimators"     "original"    "xlispstat"        "curve" 
##       Topic 16       Topic 17       Topic 18       Topic 19       Topic 20 
##      "samples"  "conditional"          "www"      "wavelet"       "fields" 
##       Topic 21       Topic 22       Topic 23       Topic 24       Topic 25 
##   "confidence"      "network"      "winbugs"       "vector"      "density" 
##       Topic 26       Topic 27       Topic 28       Topic 29       Topic 30 
## "longitudinal"   "clustering"       "choice"   "generators"    "smoothing"
</code></pre>

<pre><code class="language-r">terms(jss_TM$Gibbs)  
</code></pre>

<pre><code>##        Topic 1        Topic 2        Topic 3        Topic 4        Topic 5 
##      "wavelet"       "vector"         "item"      "density"   "population" 
##        Topic 6        Topic 7        Topic 8        Topic 9       Topic 10 
##    "procedure"        "graph"      "winbugs" "mathematical"          "gui" 
##       Topic 11       Topic 12       Topic 13       Topic 14       Topic 15 
##      "network"   "clustering"       "graphs"         "file"       "gamlss" 
##       Topic 16       Topic 17       Topic 18       Topic 19       Topic 20 
##        "power"     "lispstat"      "control"       "robust"      "designs" 
##       Topic 21       Topic 22       Topic 23       Topic 24       Topic 25 
##       "choice"         "risk"         "java"     "matrices"  "theoretical" 
##       Topic 26       Topic 27       Topic 28       Topic 29       Topic 30 
##   "confidence"        "trees"         "socr"      "copulas"   "estimators"
</code></pre>

<pre><code class="language-r">terms(jss_TM$CTM)  
</code></pre>

<pre><code>##        Topic 1        Topic 2        Topic 3        Topic 4        Topic 5 
##       "robust"     "matrices"          "www"       "kernel"   "imputation" 
##        Topic 6        Topic 7        Topic 8        Topic 9       Topic 10 
##    "criterion"        "polyk"        "vista"   "clustering" "zeroinflated" 
##       Topic 11       Topic 12       Topic 13       Topic 14       Topic 15 
##     "lispstat"         "text"         "file"       "volume"         "socr" 
##       Topic 16       Topic 17       Topic 18       Topic 19       Topic 20 
##      "control"      "winbugs"        "hydra"   "generators"       "graphs" 
##       Topic 21       Topic 22       Topic 23       Topic 24       Topic 25 
##      "designs"   "confidence"   "projection"      "network"   "microarray" 
##       Topic 26       Topic 27       Topic 28       Topic 29       Topic 30 
##   "population"      "wavelet"         "item"   "estimators" "mathematical"
</code></pre>

<pre><code class="language-r">?LDA
</code></pre>

<p><a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Wiki LDA</a></p>]]></description><link>http://localhost:2368/topic-modelling/</link><guid isPermaLink="false">7f21a63e-1142-4e4d-b84d-3b6ea34b5bb8</guid><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Sun, 23 Nov 2014 22:17:26 GMT</pubDate></item><item><title><![CDATA[Time Series Data analysis]]></title><description><![CDATA[<h1 id="timeseriesdataanalysis">Time Series Data analysis  </h1>

<p>Read the data</p>

<pre><code class="language-r">kings &lt;- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)  
kings  
</code></pre>

<pre><code>##  [1] 60 43 67 50 56 42 50 65 68 43 65 34 47 34 49 41 13 35 53 56 16 43 69
## [24] 59 48 59 86 55 68 51 33 49 67 77 81 67 71 81 68 70 77 56
</code></pre>

<p>Store in time series object</p>

<pre><code class="language-r">kingstimeseries  &lt;- ts(kings); kingstimeseries  
</code></pre>

<pre><code>## Time Series:
## Start = 1 
## End = 42 
## Frequency = 1 
##  [1] 60 43 67 50 56 42 50 65 68 43 65 34 47 34 49 41 13 35 53 56 16 43 69
## [24] 59 48 59 86 55 68 51 33 49 67 77 81 67 71 81 68 70 77 56
</code></pre>

<p>Data set of the number of births per month in New York city, from January 1946 to December 1959 (originally collected by Newton). This data is available in the file <a href="http://robjhyndman.com/tsdldata/data/nybirths.dat">http://robjhyndman.com/tsdldata/data/nybirths.dat</a> </p>

<pre><code class="language-r">births &lt;- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")  
</code></pre>

<pre><code class="language-r">birthstimeseries &lt;- ts(births, frequency=12, start=c(1946,1))  
birthstimeseries  
</code></pre>

<pre><code>##         Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    Nov    Dec
## 1946 26.663 23.598 26.931 24.740 25.806 24.364 24.477 23.901 23.175 23.227 21.672 21.870
## 1947 21.439 21.089 23.709 21.669 21.752 20.761 23.479 23.824 23.105 23.110 21.759 22.073
## 1948 21.937 20.035 23.590 21.672 22.222 22.123 23.950 23.504 22.238 23.142 21.059 21.573
## 1949 21.548 20.000 22.424 20.615 21.761 22.874 24.104 23.748 23.262 22.907 21.519 22.025
## 1950 22.604 20.894 24.677 23.673 25.320 23.583 24.671 24.454 24.122 24.252 22.084 22.991
## 1951 23.287 23.049 25.076 24.037 24.430 24.667 26.451 25.618 25.014 25.110 22.964 23.981
## 1952 23.798 22.270 24.775 22.646 23.988 24.737 26.276 25.816 25.210 25.199 23.162 24.707
## 1953 24.364 22.644 25.565 24.062 25.431 24.635 27.009 26.606 26.268 26.462 25.246 25.180
## 1954 24.657 23.304 26.982 26.199 27.210 26.122 26.706 26.878 26.152 26.379 24.712 25.688
## 1955 24.990 24.239 26.721 23.475 24.767 26.219 28.361 28.599 27.914 27.784 25.693 26.881
## 1956 26.217 24.218 27.914 26.975 28.527 27.139 28.982 28.169 28.056 29.136 26.291 26.987
## 1957 26.589 24.848 27.543 26.896 28.878 27.390 28.065 28.141 29.048 28.484 26.634 27.735
## 1958 27.132 24.924 28.963 26.589 27.931 28.009 29.229 28.759 28.405 27.945 25.912 26.619
## 1959 26.076 25.286 27.660 25.951 26.398 25.565 28.865 30.000 29.261 29.012 26.992 27.897
</code></pre>

<p>The file <a href="http://robjhyndman.com/tsdldata/data/fancy.dat">http://robjhyndman.com/tsdldata/data/fancy.dat</a> contains monthly sales for a souvenir shop at a beach resort town in Queensland, Australia, for January 1987-December 1993 (original data from Wheelwright and Hyndman, 1998).</p>

<pre><code class="language-r">souvenir &lt;- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")  
</code></pre>

<pre><code class="language-r">souvenirtimeseries &lt;- ts(souvenir, frequency=12, start=c(1987,1))  
souvenirtimeseries  
</code></pre>

<pre><code>##            Jan       Feb       Mar       Apr       May       Jun       Jul            Aug       Sep       Oct       Nov       Dec
## 1987   1664.81   2397.53   2840.71   3547.29   3752.96   3714.74   4349.61 1987   3566.34   5021.82   6423.48   7600.60  19756.21
## 1988   2499.81   5198.24   7225.14   4806.03   5900.88   4951.34   6179.12 1988   4752.15   5496.43   5835.10  12600.08  28541.72
## 1989   4717.02   5702.63   9957.58   5304.78   6492.43   6630.80   7349.62 1989   8176.62   8573.17   9690.50  15151.84  34061.01
## 1990   5921.10   5814.58  12421.25   6369.77   7609.12   7224.75   8121.22 1990   7979.25   8093.06   8476.70  17914.66  30114.41
## 1991   4826.64   6470.23   9638.77   8821.17   8722.37  10209.48  11276.55 1991  12552.22  11637.39  13606.89  21822.11  45060.69
## 1992   7615.03   9849.69  14558.40  11587.33   9332.56  13082.09  16732.78 1992  19888.61  23933.38  25391.35  36024.80  80721.71
## 1993  10243.24  11266.88  21826.84  17357.33  15997.79  18601.53  26155.15 1993  28586.52  30505.41  30821.33  46634.38 104660.67
</code></pre>

<p>Plotting Time Series</p>

<pre><code class="language-r">plot.ts(kingstimeseries)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-7-1.png" alt="plot of chunk unnamed-chunk-7"></p>

<p>This time series could probably be described using an <code>additive model</code>, since the random fluctuations in the data are roughly constant in size over time.</p>

<p>Plot Birth Time Series  </p>

<pre><code class="language-r">plot.ts(birthstimeseries)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-8-1.png" alt="plot of chunk unnamed-chunk-8"></p>

<p>There seems to be <code>seasonal variation</code> in the number of births per month: there is a peak every summer, and a trough every winter. Again, it seems that this time series could probably be described using an <code>additive model</code>, as the seasonal fluctuations are roughly constant in size over time and do not seem to depend on the level of the time series, and the random fluctuations also seem to be roughly constant in size over time.</p>

<p>Plot souvenir time series data  </p>

<pre><code class="language-r">plot.ts(souvenirtimeseries)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-9-1.png" alt="plot of chunk unnamed-chunk-9"></p>

<p>an <code>additive model</code> is not appropriate for describing this time series, since the size of the seasonal fluctuations and random fluctuations seem to increase with the level of the time series. Thus, we may need to <code>transform</code> the time series in order to get a transformed time series that can be described using an additive model. </p>

<pre><code class="language-r">logsouvenirtimeseries &lt;- log(souvenirtimeseries)  
plot.ts(logsouvenirtimeseries)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-10-1.png" alt="plot of chunk unnamed-chunk-10"></p>

<p>The size of the seasonal fluctuations and random fluctuations in the log-transformed time series seem to be roughly constant over time, and do not depend on the level of the time series. Thus, the log- transformed time series can probably be described using an additive model.</p>

<h2 id="decomposingtimeseries">Decomposing Time Series  </h2>

<p>Separating it into its constituent components, which are usually a <code>trend</code> component and an <code>irregular</code> component, and if it is a seasonal time series, a seasonal component.</p>

<p>Decomposing Non-Seasonal Data</p>

<p>A non-seasonal time series consists of a <code>trend</code> component and an <code>irregular</code> component.</p>

<p>To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a <code>smoothing method</code>, such as calculating the <code>simple moving average</code> of the time series.</p>

<p>The time series of the age of death of 42 successive kings of England appears is non-seasonal, and can probably be described using an additive model, since the random fluctuations in the data are roughly constant in size over time:</p>

<pre><code class="language-r">par(mfrow=c(2,1))  
plot.ts(kingstimeseries)  
kingstimeseriesSMA3  &lt;- SMA(kingstimeseries, n=3)  
plot.ts(kingstimeseriesSMA3)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-12-1-1.png" alt="plot of chunk unnamed-chunk-12"></p>

<p>There still appears to be quite a lot of random fluctuations in the time series smoothed using a simple moving average of order 3. Thus, to estimate the trend component more accurately, we might want to try smoothing the data with a simple moving average of a higher order. This takes a little bit of trial-and-error, to find the right amount of smoothing. For example, we can try using a simple moving average of order 8:</p>

<pre><code class="language-r">par(mfrow=c(2,1))  
plot.ts(kingstimeseries)  
kingstimeseriesSMA3  &lt;- SMA(kingstimeseries, n=8)  
plot.ts(kingstimeseriesSMA3)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13"></p>

<p>The data smoothed with a <code>simple moving average</code> of order 8 gives a clearer picture of the trend component, and we can see that the age of death of the English kings seems to have decreased from about 55 years old to about 38 years old during the reign of the first 20 kings, and then increased after that to about 73 years old by the end of the reign of the 40th king in the time series.</p>

<h3 id="decomposingseasonaldata">Decomposing Seasonal Data</h3>

<p>A seasonal time series consists of a trend component, a seasonal component and an irregular component. Decom- posing the time series means separating the time series into these three components: that is, estimating these three components.</p>

<p>To estimate the trend component and seasonal component of a seasonal time series that can be described using an additive model, we can use the “decompose()” function in R. This function estimates the trend, seasonal, and irregular components of a time series that can be described using an additive model.</p>

<p>The function <code>decompose()</code> returns a list object as its result, where the estimates of the seasonal component, trend component and irregular component are stored in named elements of that list objects, called <code>seasonal</code>, <code>trend</code>, and <code>random</code> respectively. <br>
For example, as discussed above, the time series of the number of births per month in New York city is seasonal with a peak every summer and trough every winter, and can probably be described using an additive model since the seasonal and random fluctuations seem to be roughly constant in size over time:</p>

<pre><code class="language-r">plot.ts(birthstimeseries)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-14.png" alt="plot of chunk unnamed-chunk-14"></p>

<p>To estimate the trend, seasonal and irregular components of this time series, we type:</p>

<pre><code class="language-r">birthstimeseriescomponents &lt;- decompose(birthstimeseries)  
</code></pre>

<p>The estimated values of the seasonal, trend and irregular components are now stored in variables <code>birthstimeseriescomponents$seasonal</code>, <code>birthstimeseriescomponents$trend</code> and <code>birthstimeseriescomponents$random</code>. For example, we can print out the estimated values of the seasonal component by typing:</p>

<pre><code class="language-r">birthstimeseriescomponents$seasonal  
</code></pre>

<pre><code>##             Jan        Feb        Mar        Apr        May        Jun             Jul        Aug        Sep        Oct        Nov        Dec
## 1946 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1946  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1947 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1947  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1948 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1948  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1949 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1949  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1950 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1950  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1951 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1951  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1952 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1952  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1953 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1953  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1954 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1954  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1955 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1955  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1956 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1956  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1957 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1957  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1958 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1958  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
## 1959 -0.6771947 -2.0829607  0.8625232 -0.8016787  0.2516514 -0.1532556 1959  1.4560457  1.1645938  0.6916162  0.7752444 -1.1097652 -0.3768197
</code></pre>

<p>The estimated seasonal factors are given for the months January-December, and are the same for each year. The largest seasonal factor is for July (about 1.46), and the lowest is for February (about -2.08), indicating that there seems to be a peak in births in July and a trough in births in February each year.</p>

<p>We can plot the estimated trend, seasonal, and irregular components of the time series by using the “plot()” function, for example:</p>

<pre><code class="language-r">plot(birthstimeseriescomponents)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-17-1.png" alt="plot of chunk unnamed-chunk-17"></p>

<p>We see that the estimated trend component shows a small decrease from about 24 in 1947 to about 22 in 1948, followed by a steady increase from then on to about 27 in 1959.</p>

<h3 id="seasonalityadjusting">Seasonality Adjusting</h3>

<p>If you have a seasonal time series that can be described using an additive model, you can seasonally adjust the time series by estimating the seasonal component, and subtracting the estimated seasonal component from the original time series. </p>

<p>For example, to seasonally adjust the time series of the number of births per month in New York city, we can estimate the seasonal component using “decompose()”, and then subtract the seasonal component from the original time series:</p>

<pre><code class="language-r">birthstimeseriescomponents &lt;- decompose(birthstimeseries)  
birthstimeseriesseasonallyadjusted &lt;- birthstimeseries - birthstimeseriescomponents$seasonal  
</code></pre>

<pre><code class="language-r">par(mfrow=c(3,1))  
plot(birthstimeseries)  
plot(birthstimeseriescomponents$seasonal)  
plot(birthstimeseriesseasonallyadjusted)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-19-1.png" alt="plot of chunk unnamed-chunk-19"></p>

<p>You can see that the seasonal variation has been removed from the seasonally adjusted time series. The seasonally adjusted time series now just contains the trend component and an irregular component.</p>

<h2 id="forecastsusingexponentialsmoothing">Forecasts using Exponential Smoothing  </h2>

<p>Exponential smoothing can be used to make <strong>short-term forecasts</strong> for time series data.</p>

<h3 id="simpleexponentialsmoothing">Simple Exponential Smoothing</h3>

<p>If you have a time series that can be described using an additive model with <strong>constant level and no seasonality</strong>, you can use simple exponential smoothing to make short-term forecasts.</p>

<p>The simple exponential smoothing method provides a way of estimating the <code>level</code> at the current time point. Smoothing is controlled by the parameter <code>alpha</code>; for the <code>estimate of the level</code> at the current time point. The value of <code>alpha</code>; lies between <strong>0</strong> and <strong>1</strong>. Values of <code>alpha</code> that are close to <strong>0</strong> mean that <strong>little weight</strong> is placed on the <strong>most recent observations</strong> when making forecasts of future values.</p>

<p>For example, the file <a href="http://robjhyndman.com/tsdldata/hurst/precip1.dat">http://robjhyndman.com/tsdldata/hurst/precip1.dat</a> contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994). We can read the data into R and plot it by typing:</p>

<pre><code class="language-r">rain &lt;- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)

rainseries &lt;- ts(rain,start=c(1813))  
plot.ts(rainseries)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-20-1.png" alt="plot of chunk unnamed-chunk-20"></p>

<p>You can see from the plot that there is roughly <strong>constant level</strong> (the mean stays constant at about 25 inches). The random fluctuations in the time series seem to be roughly constant in size over time, so it is probably appropriate to describe the data using an <strong>additive model</strong>. Thus, we can make forecasts using simple exponential smoothing.</p>

<p>To make forecasts using simple exponential smoothing in R, we can fit a simple exponential smoothing predictive model using the <code>“HoltWinters()”</code> function in R. To use <code>HoltWinters()</code> for simple exponential smoothing, we need to set the parameters <code>beta=FALSE</code> and <code>gamma=FALSE</code> in the <code>HoltWinters()</code> function (the beta and gamma parameters are used for Holt’s exponential smoothing, or Holt-Winters exponential smoothing, as described below).</p>

<p>The HoltWinters() function returns a list variable, that contains several named elements. <br>
For example, to use simple exponential smoothing to make forecasts for the time series of annual rainfall in London, we type</p>

<pre><code class="language-r">rainseriesforecasts &lt;- HoltWinters(rainseries, beta=FALSE, gamma=FALSE)  
rainseriesforecasts  
</code></pre>

<pre><code>## Holt-Winters exponential smoothing without trend and without seasonal component.
## 
## Call:
## HoltWinters(x = rainseries, beta = FALSE, gamma = FALSE)
## 
## Smoothing parameters:
##  alpha: 0.02412151
##  beta : FALSE
##  gamma: FALSE
## 
## Coefficients:
##       [,1]
## a 24.67819
</code></pre>

<p>The output of HoltWinters() tells us that the estimated value of the <code>alpha</code> parameter is about 0.024. This is very close to zero, telling us that the forecasts are based on both recent and less recent observations (although somewhat more weight is placed on recent observations).</p>

<p>By default, HoltWinters() just makes forecasts for the same time period covered by our original time series. In this case, our original time series included rainfall for London from 1813-1912, so the forecasts are also for 1813-1912.</p>

<p>In the example above, we have stored the output of the HoltWinters() function in the list variable “rainseriesforecasts”. The <code>forecasts</code> made by HoltWinters() are stored in a named element of this list variable called “fitted”, so we can get their values by typing:</p>

<pre><code class="language-r">rainseriesforecasts$fitted  
</code></pre>

<pre><code>## Time Series:
## Start = 1814 
## End = 1912 
## Frequency = 1 
##          xhat    level
## 1814 23.56000 23.56000
## 1815 23.62054 23.62054
## 1816 23.57808 23.57808
## 1817 23.76290 23.76290
## 1818 23.76017 23.76017
## 1819 23.76306 23.76306
## 1820 23.82691 23.82691
## 1821 23.79900 23.79900
## 1822 23.98935 23.98935
## 1823 23.98623 23.98623
## 1824 23.98921 23.98921
## 1825 24.19282 24.19282
## 1826 24.17032 24.17032
## 1827 24.13171 24.13171
## 1828 24.10442 24.10442
## 1829 24.19549 24.19549
## 1830 24.22261 24.22261
## 1831 24.24329 24.24329
## 1832 24.32812 24.32812
## 1833 24.21938 24.21938
## 1834 24.23290 24.23290
## 1835 24.13369 24.13369
## 1836 24.13867 24.13867
## 1837 24.21782 24.21782
## 1838 24.10257 24.10257
## 1839 24.04293 24.04293
## 1840 24.12608 24.12608
## 1841 24.01280 24.01280
## 1842 24.18448 24.18448
## 1843 24.15808 24.15808
## 1844 24.19889 24.19889
## 1845 24.16153 24.16153
## 1846 24.12748 24.12748
## 1847 24.18133 24.18133
## 1848 24.02499 24.02499
## 1849 24.16454 24.16454
## 1850 24.13476 24.13476
## 1851 24.01621 24.01621
## 1852 23.93453 23.93453
## 1853 24.20964 24.20964
## 1854 24.25018 24.25018
## 1855 24.11509 24.11509
## 1856 24.08964 24.08964
## 1857 24.04430 24.04430
## 1858 23.99933 23.99933
## 1859 23.87319 23.87319
## 1860 23.97780 23.97780
## 1861 24.17710 24.17710
## 1862 24.13110 24.13110
## 1863 24.21405 24.21405
## 1864 24.15075 24.15075
## 1865 23.97658 23.97658
## 1866 24.10933 24.10933
## 1867 24.29001 24.29001
## 1868 24.33729 24.33729
## 1869 24.31468 24.31468
## 1870 24.34134 24.34134
## 1871 24.26847 24.26847
## 1872 24.28659 24.28659
## 1873 24.51752 24.51752
## 1874 24.47295 24.47295
## 1875 24.33660 24.33660
## 1876 24.43558 24.43558
## 1877 24.47717 24.47717
## 1878 24.56625 24.56625
## 1879 24.79573 24.79573
## 1880 25.01341 25.01341
## 1881 25.14045 25.14045
## 1882 25.20750 25.20750
## 1883 25.25411 25.25411
## 1884 25.23351 25.23351
## 1885 25.11571 25.11571
## 1886 25.15248 25.15248
## 1887 25.19729 25.19729
## 1888 25.05286 25.05286
## 1889 25.11768 25.11768
## 1890 25.08710 25.08710
## 1891 24.99407 24.99407
## 1892 25.07019 25.07019
## 1893 25.01085 25.01085
## 1894 24.88515 24.88515
## 1895 24.95884 24.95884
## 1896 24.87469 24.87469
## 1897 24.84201 24.84201
## 1898 24.79420 24.79420
## 1899 24.62284 24.62284
## 1900 24.57259 24.57259
## 1901 24.54141 24.54141
## 1902 24.48421 24.48421
## 1903 24.39631 24.39631
## 1904 24.72686 24.72686
## 1905 24.62852 24.62852
## 1906 24.58852 24.58852
## 1907 24.58059 24.58059
## 1908 24.54271 24.54271
## 1909 24.52166 24.52166
## 1910 24.57541 24.57541
## 1911 24.59433 24.59433
## 1912 24.59905 24.59905
</code></pre>

<p>We can plot the original time series against the forecasts by typing:</p>

<pre><code class="language-r">plot(rainseriesforecasts)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-23-1-1.png" alt="plot of chunk unnamed-chunk-23"></p>

<p>The plot shows the original time series in black, and the forecasts as a red line. The time series of forecasts is much <strong>smoother</strong> than the time series of the original data here.</p>

<p>As a measure of the <strong>accuracy</strong> of the forecasts, we can calculate the <strong>sum of squared errors</strong> for the in-sample forecast errors, that is, the forecast errors for the time period covered by our original time series. The sum-of-squared-errors is stored in a named element of the list variable <code>“rainseriesforecasts”</code> called <code>“SSE”</code>, so we can get its value by typing:</p>

<pre><code class="language-r">rainseriesforecasts$SSE  
</code></pre>

<pre><code>## [1] 1828.855
</code></pre>

<p>It is common in simple exponential smoothing to use the first value in the time series as the initial value for the level. For example, in the time series for rainfall in London, the first value is 23.56 (inches) for rainfall in 1813. You can specify the initial value for the level in the HoltWinters() function by using the “l.start” parameter. For example, to make forecasts with the initial value of the level set to 23.56, we type:</p>

<pre><code class="language-r">HoltWinters(rainseries, beta=FALSE, gamma=FALSE, l.start=23.56)  
</code></pre>

<pre><code>## Holt-Winters exponential smoothing without trend and without seasonal component.
## 
## Call:
## HoltWinters(x = rainseries, beta = FALSE, gamma = FALSE, l.start = 23.56)
## 
## Smoothing parameters:
##  alpha: 0.02412151
##  beta : FALSE
##  gamma: FALSE
## 
## Coefficients:
##       [,1]
## a 24.67819
</code></pre>

<p>As explained above, by default HoltWinters() just makes forecasts for the time period covered by the original data, which is 1813-1912 for the rainfall time series. We can make forecasts for further time points by using the “forecast.HoltWinters()” function in the R “forecast” package. To use the forecast.HoltWinters() function, we first need to install the “forecast” R package</p>

<pre><code class="language-r">library("forecast")  
</code></pre>

<p>When using the <code>forecast.HoltWinters()</code> function, as its first argument (input), you pass it the <strong>predictive model</strong> that you have already fitted using the <code>HoltWinters()</code> function. For example, in the case of the rainfall time series, we stored the predictive model made using <code>HoltWinters()</code> in the variable <code>“rainseriesforecasts”</code>. You specify how many further time points you want to make forecasts for by using the <code>“h”</code> parameter in <code>forecast.HoltWinters()</code>. For example, to make a forecast of rainfall for the years 1814-1820 (<code>8</code> more years) using <code>forecast.HoltWinters()</code>, we type:</p>

<pre><code class="language-r">rainseriesforecasts2 &lt;- forecast.HoltWinters(rainseriesforecasts, h=8)  
rainseriesforecasts2  
</code></pre>

<pre><code>##      Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
## 1913       24.67819 19.17493 30.18145 16.26169 33.09470
## 1914       24.67819 19.17333 30.18305 16.25924 33.09715
## 1915       24.67819 19.17173 30.18465 16.25679 33.09960
## 1916       24.67819 19.17013 30.18625 16.25434 33.10204
## 1917       24.67819 19.16853 30.18785 16.25190 33.10449
## 1918       24.67819 19.16694 30.18945 16.24945 33.10694
## 1919       24.67819 19.16534 30.19105 16.24701 33.10938
## 1920       24.67819 19.16374 30.19265 16.24456 33.11182
</code></pre>

<p>The forecast.HoltWinters() function gives you the forecast for a year, a 80% prediction interval for the forecast, and a 95% prediction interval for the forecast. For example, the forecasted rainfall for 1920 is about 24.68 inches, with a 95% prediction interval of (16.24, 33.11).</p>

<p>To plot the predictions made by forecast.HoltWinters(), we can use the “plot.forecast()” function:</p>

<pre><code class="language-r">plot.forecast(rainseriesforecasts2)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-28-1-2.png" alt="plot of chunk unnamed-chunk-28"></p>

<p>The ‘forecast errors’ are calculated as the observed values minus predicted values, for each time point. We can only calculate the forecast errors for the time period covered by our original time series, which is 1813-1912 for the rainfall data. As mentioned above, one measure of the accuracy of the predictive model is the sum-of-squared- errors (SSE) for the in-sample forecast errors.</p>

<p>The in-sample forecast errors are stored in the named element “residuals” of the list variable returned by <code>forecast.HoltWinters()</code>. If the predictive model cannot be improved upon, there should be no correlations between forecast errors for successive predictions. In other words, if there are correlations between forecast errors for successive predictions, it is likely that the simple exponential smoothing forecasts could be improved upon by another forecasting technique.</p>

<p>To figure out whether this is the case, we can obtain a <code>correlogra  m</code> of the in-sample forecast errors for lags 1-20. We can calculate a  <code>correlogram</code> of the forecast errors using the <code>“acf()”</code> function in R. To specify the maximum lag that we want to look at, we use the <code>“lag.max”</code> parameter in acf().</p>

<p>For example, to calculate a correlogram of the in-sample forecast errors for the London rainfall data for lags 1-20, we type:</p>

<pre><code class="language-r">acf(rainseriesforecasts2$residuals, lag.max=20)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-29-1-1.png" alt="plot of chunk unnamed-chunk-29"></p>

<pre><code class="language-r">rm(list = ls())  
</code></pre>]]></description><link>http://localhost:2368/time-series-data-analysis/</link><guid isPermaLink="false">4b368374-1bfd-437d-8f38-36018c155bdd</guid><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Sun, 23 Nov 2014 20:53:27 GMT</pubDate></item><item><title><![CDATA[RMySQL]]></title><description><![CDATA[<h1 id="rmysql">RMySQL  </h1>

<pre><code class="language-r">library(RMySQL)  
</code></pre>

<p>Connect to the database  </p>

<pre><code class="language-r">ucscDb &lt;- dbConnect(MySQL(),user="genome",  
                    host="genome-mysql.cse.ucsc.edu")
result &lt;- dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);  
</code></pre>

<pre><code>## [1] TRUE
</code></pre>

<p>See the databases returned from previous query  </p>

<pre><code class="language-r">result[1:20,]  
</code></pre>

<pre><code>##  [1] "information_schema" "ailMel1"            "allMis1"           
##  [4] "anoCar1"            "anoCar2"            "anoGam1"           
##  [7] "apiMel1"            "apiMel2"            "aplCal1"           
## [10] "balAcu1"            "bosTau2"            "bosTau3"           
## [13] "bosTau4"            "bosTau5"            "bosTau6"           
## [16] "bosTau7"            "bosTauMd3"          "braFlo1"           
## [19] "caeJap1"            "caePb1"
</code></pre>

<p>Connect to the <code>hg19</code> database  </p>

<pre><code class="language-r">hg19 &lt;- dbConnect(MySQL(),user="genome", db="hg19",  
                    host="genome-mysql.cse.ucsc.edu")
allTables &lt;- dbListTables(hg19)  
length(allTables)  
</code></pre>

<p>Number of tables in this database  </p>

<pre><code>## [1] 11015
</code></pre>

<p>First 5 tables.  </p>

<pre><code class="language-r">allTables[1:5]  
</code></pre>

<pre><code>## [1] "HInv"         "HInvGeneMrna" "acembly"      "acemblyClass"
## [5] "acemblyPep"
</code></pre>

<p>Get the columns(fields) of the <code>affyU133Plus2</code> table</p>

<pre><code class="language-r">dbListFields(hg19,"affyU133Plus2")  
</code></pre>

<pre><code>##  [1] "bin"         "matches"     "misMatches"  "repMatches"  "nCount"     
##  [6] "qNumInsert"  "qBaseInsert" "tNumInsert"  "tBaseInsert" "strand"     
## [11] "qName"       "qSize"       "qStart"      "qEnd"        "tName"      
## [16] "tSize"       "tStart"      "tEnd"        "blockCount"  "blockSizes" 
## [21] "qStarts"     "tStarts"
</code></pre>

<p>Get the number of rows from the table</p>

<pre><code class="language-r">dbGetQuery(hg19, "select count(*) from affyU133Plus2")  
</code></pre>

<pre><code>##   count(*)
## 1    58463
</code></pre>

<p>Get first few rows of the table. </p>

<pre><code class="language-r">affyData &lt;- dbReadTable(hg19, "affyU133Plus2")  
head(affyData)  
</code></pre>

<pre><code>##   bin matches misMatches repMatches nCount qNumInsert qBaseInsert tNumInsert tBaseInsert strand        qName qSize qStart qEnd tName     tSize tStart  tEnd blockCount                                                                 blockSizes                                                                                                qStarts                                                                                                                                    tStarts
## 1 585     530          4          0     23          3          41          3         898      -  225995_x_at   637      5  603  chr1 249250621  14361 15816          5                                                          93,144,229,70,21,                                                                                    34,132,278,541,611,                                                                                                             14361,14454,14599,14968,15795,
## 2 585    3355         17          0    109          9          67          9       11621      -  225035_x_at  3635      0 3548  chr1 249250621  14381 29483         17              73,375,71,165,303,360,198,661,201,1,260,250,74,73,98,155,163,                        87,165,540,647,818,1123,1484,1682,2343,2545,2546,2808,3058,3133,3206,3317,3472,                                     14381,14454,14969,15075,15240,15543,15903,16104,16853,17054,17232,17492,17914,17988,18267,24736,29320,
## 3 585    4156         14          0     83         16          18          2          93      -  226340_x_at  4318      3 4274  chr1 249250621  14399 18745         18                 690,10,32,33,376,4,5,15,5,11,7,41,277,859,141,51,443,1253,                   44,735,746,779,813,1190,1195,1201,1217,1223,1235,1243,1285,1564,2423,2565,2617,3062,                               14399,15089,15099,15131,15164,15540,15544,15549,15564,15569,15580,15587,15628,15906,16857,16998,17049,17492,
## 4 585    4667          9          0     68         21          42          3        5743      - 1557034_s_at  4834     48 4834  chr1 249250621  14406 24893         23 99,352,286,24,49,14,6,5,8,149,14,44,98,12,10,355,837,59,8,1500,133,624,58, 0,99,452,739,764,814,829,836,842,851,1001,1016,1061,1160,1173,1184,1540,2381,2441,2450,3951,4103,4728, 14406,20227,20579,20865,20889,20938,20952,20958,20963,20971,21120,21134,21178,21276,21288,21298,21653,22492,22551,22559,24059,24211,24835,
## 5 585    5180         14          0    167         10          38          1          29      -    231811_at  5399      0 5399  chr1 249250621  19688 25078         11                                       131,26,1300,6,4,11,4,7,358,3359,155,                                                     0,132,159,1460,1467,1472,1484,1489,1497,1856,5244,                                                                         19688,19819,19845,21145,21151,21155,21166,21170,21177,21535,24923,
## 6 585     468          5          0     14          0           0          0           0      -    236841_at   487      0  487  chr1 249250621  27542 28029          1                                                                       487,                                                                                                     0,                                                                                                                                     27542,
</code></pre>

<pre><code class="language-r">query &lt;- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")  
affyMis &lt;- fetch(query); quantile(affyMis$misMatches)  
</code></pre>

<pre><code>##   0%  25%  50%  75% 100% 
##    1    1    2    2    3
</code></pre>

<p>define number of rows to extract.</p>

<pre><code class="language-r">affyMisSmall &lt;- fetch(query,n=10); dbClearResult(query);  
</code></pre>

<pre><code>## [1] TRUE
</code></pre>

<p>Dimensions of the result set</p>

<pre><code class="language-r">dim(affyMisSmall)  
</code></pre>

<pre><code>## [1] 10 22
</code></pre>

<p>Dont forget to disconnect the database. </p>

<pre><code class="language-r">dbDisconnect(hg19)  
</code></pre>

<pre><code>## [1] TRUE
</code></pre>]]></description><link>http://localhost:2368/rmysql/</link><guid isPermaLink="false">a7d91582-4183-49bd-bbe6-502c42de4352</guid><category><![CDATA[Rstats]]></category><category><![CDATA[Coursera]]></category><category><![CDATA[MySQL]]></category><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Sun, 23 Nov 2014 20:29:22 GMT</pubDate></item><item><title><![CDATA[Statistical text processing]]></title><description><![CDATA[<p>Source - <a href="http://www.r-datacollection.com/materials/ch-10-textmining/ch-10-textmining.r">http://www.r-datacollection.com/materials/ch-10-textmining/ch-10-textmining.r</a></p>

<p>Statistical Text Processing</p>

<pre><code class="language-{r}"># load packages
library(RCurl)  
library(XML)  
library(stringr)  
library(tm)  
library(SnowballC)  
library(RWeka)  
library(RTextTools)  
library(topicmodels)  
</code></pre>

<h6 id="101therunningexampleclassifyingpressreleasesofthebritishgovernment">10.1 The running example: Classifying press releases of the British government</h6>

<p>Downloading all results</p>

<pre><code class="language-r">all_links &lt;- character()  
new_results &lt;- 'government/announcements?keywords=&amp;announcement_type_option=press-releases&amp;topics[]=all&amp;departments[]=all&amp;world_locations[]=all&amp;from_date=&amp;to_date=01%2F07%2F2010'  
signatures = system.file("CurlSSL", cainfo = "cacert.pem", package = "RCurl")  
while(length(new_results) &gt; 0){  
    new_results &lt;- str_c("https://www.gov.uk/", new_results)
    results &lt;- getURL(new_results, cainfo = signatures)
    results_tree &lt;- htmlParse(results)
    all_links &lt;- c(all_links, xpathSApply(results_tree, "//li[@id]//a", xmlGetAttr, "href"))
    new_results &lt;- xpathSApply(results_tree, "//nav[@id='show-more-documents']//li[@class='next']//a", xmlGetAttr, "href")
}
</code></pre>

<p>Check the entries</p>

<pre><code class="language-r">all_links[1]  
</code></pre>

<pre><code>## [1] "/government/news/the-turks-and-caicos-islands-good-governance"
</code></pre>

<pre><code class="language-r">length(all_links)  
</code></pre>

<pre><code>## [1] 751
</code></pre>

<p>Download all press releases</p>

<pre><code class="language-r">if (!file.exists("Press_Releases")) dir.create("Press_Releases")  
for(i in 1:length(all_links)){  
    url &lt;- str_c("https://www.gov.uk", all_links[i])
    tmp &lt;- getURL(url, cainfo = signatures)
    write(tmp, str_c("Press_Releases/", i, ".html"))
}
</code></pre>

<p>Check results</p>

<p>Number of files in the directory  </p>

<pre><code class="language-r">length(list.files("Press_Releases"))  
</code></pre>

<pre><code>## [1] 535
</code></pre>

<p>First three files.</p>

<pre><code class="language-r">list.files("Press_Releases")[1:3]  
</code></pre>

<pre><code>## [1] "1.html"   "10.html"  "100.html"
</code></pre>

<h6 id="102processingtextualdata">10.2 Processing Textual Data</h6>

<h6 id="1021largescaletextoperationsthetmpackage">10.2.1 Large-scale text operations - the tm package</h6>

<p>Get press release  </p>

<pre><code class="language-r">tmp &lt;- readLines("Press_Releases/1.html")  
tmp &lt;- str_c(tmp, collapse = "") ## Convert to single string  
tmp &lt;- htmlParse(tmp)  
release &lt;- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)  
release  
</code></pre>

<pre><code>## [1] "                  In welcoming Mr Parnell, the Minister spoke of the Government’s wish to build a new dynamic relationship with the Overseas Territories and to see the Turks and Caicos Islands (TCI) on a sustainable and stable footing.  Mr Parnell shared with the Minister his assessment of the current situation in the Territory, covering a range of issues including the economy, investment promotion and elections.  Speaking after the meeting, Henry Bellingham said: “Mr Parnell and I enjoyed a brief but frank exchange on issues of concern to TCI. I look forward to visiting the Turks and Caicos Islands in the future”.   Search the news archive            "
</code></pre>

<p>Get meta information (organisation and date of publication)  </p>

<pre><code class="language-r">organisation &lt;- xpathSApply(tmp, "//dd[@class='from']", xmlValue)  
organisation &lt;- xpathSApply(tmp, "//a[@class='organisation-link'][1]", xmlValue)  
organisation  
</code></pre>

<pre><code>## [1] "Foreign &amp; Commonwealth Office"
</code></pre>

<pre><code class="language-r">publication &lt;- xpathSApply(tmp, "//div[@class='inner-heading']/h1", xmlValue)  
publication  
</code></pre>

<pre><code>## [1] "The Turks and Caicos Islands: Good Governance"
</code></pre>

<p>Create a corpus from a vector  </p>

<pre><code class="language-r">release_corpus &lt;- Corpus(VectorSource(release))  
release_corpus  
</code></pre>

<pre><code>## A corpus with 1 text document
</code></pre>

<p>Setting the meta information</p>

<pre><code class="language-r">meta(release_corpus[[1]], "organisation") &lt;- organisation[1]  
meta(release_corpus[[1]], "publication") &lt;- publication  
meta(release_corpus[[1]])  
</code></pre>

<pre><code>## Available meta data pairs are:
##   Author       : 
##   DateTimeStamp: 2014-11-21 01:39:41
##   Description  : 
##   Heading      : 
##   ID           : 1
##   Language     : en
##   Origin       : 
## User-defined local meta data pairs are:
## $organisation
## [1] "Foreign &amp; Commonwealth Office"
## 
## $publication
## [1] "The Turks and Caicos Islands: Good Governance"
</code></pre>

<p>Load remaining documents  </p>

<pre><code class="language-r">n &lt;- 1  
for(i in 2:length(list.files("Press_Releases/"))){  
    tmp &lt;- readLines(str_c("Press_Releases/", i, ".html"))
    tmp &lt;- str_c(tmp, collapse = "")
    tmp &lt;- htmlParse(tmp)
    release &lt;- xpathSApply(tmp, "//div[@class='block-4']", xmlValue)
    organisation &lt;- xpathSApply(tmp, "//dd[@class='from']", xmlValue)
    publication &lt;- xpathSApply(tmp, "//div[@class='inner-heading']/h1", xmlValue)
    if(length(release) != 0){
        n &lt;- n + 1
        #tmp_corpus &lt;- Corpus(VectorSource(release))
        release_corpus[n] &lt;- Corpus(VectorSource(release))
        meta(release_corpus[[n]], "organisation") &lt;- organisation[1]
        meta(release_corpus[[n]], "publication") &lt;- publication
    }
}
release_corpus  
</code></pre>

<pre><code>## A corpus with 545 text documents
</code></pre>

<p>Inspect meta data <br>
<code>prescindMeta</code> is removed in <code>tm 0.6</code></p>

<pre><code class="language-r">meta_data &lt;- prescindMeta(release_corpus, c("organisation", "publication"))  
table(as.character(meta_data[, "organisation"]))  
</code></pre>

<pre><code>## 
##                                   Cabinet Office 
##                                               18 
##     Department for Business, Innovation &amp; Skills 
##                                               49 
##  Department for Communities and Local Government 
##                                               15 
##            Department for Culture, Media &amp; Sport 
##                                               11 
##                         Department for Education 
##                                                3 
## Department for Environment, Food &amp; Rural Affairs 
##                                               27 
##                         Department for Transport 
##                                               13 
##                 Department for Work and Pensions 
##                                               14 
##            Department of Energy &amp; Climate Change 
##                                               14 
##                   Deputy Prime Minister's Office 
##                                                3 
##              Driver and Vehicle Licensing Agency 
##                                                4 
##                    Foreign &amp; Commonwealth Office 
##                                              151 
##                                      HM Treasury 
##                                               10 
##                                      Home Office 
##                                                8 
##                              Ministry of Defence 
##                                              118 
##       Prime Minister's Office, 10 Downing Street 
##                                               48 
##                                  Scotland Office 
##                                                9 
##             Vehicle and Operator Services Agency 
##                                                4 
##                                     Wales Office 
##                                               26
</code></pre>

<p>Filtering the corpus</p>

<pre><code class="language-r">release_corpus  
release_corpus &lt;- release_corpus[sFilter(release_corpus, "  
                                        organisation == 'Department for Business, Innovation &amp; Skills' |
                                        organisation == 'Department for Communities and Local Government' |
                                        organisation == 'Department for Environment, Food &amp; Rural Affairs' |
                                        organisation == 'Foreign &amp; Commonwealth Office' |
                                        organisation == 'Ministry of Defence' |
                                        organisation == 'Wales Office'")]
release_corpus  
</code></pre>

<pre><code class="language-r">afgh &lt;- tm_filter(release_corpus, FUN = function(x) any(str_detect(x, "Afghanistan")))  
afgh  
</code></pre>

<pre><code>## A corpus with 101 text documents
</code></pre>

<h5 id="1022buildingatermdocumentmatrix">10.2.2 Building a term-document matrix</h5>

<pre><code class="language-r">tdm &lt;- TermDocumentMatrix(release_corpus)  
tdm  
</code></pre>

<pre><code>## A term-document matrix (25165 terms, 545 documents)
## 
## Non-/sparse entries: 111988/13602937
## Sparsity           : 99%
## Maximal term length: 111 
## Weighting          : term frequency (tf)
</code></pre>

<h5 id="1023datacleansing">10.2.3 Data cleansing</h5>

<p>Remove numbers</p>

<pre><code class="language-r">release_corpus &lt;- tm_map(release_corpus, removeNumbers)  
</code></pre>

<p>Remove punctuation</p>

<pre><code class="language-r">release_corpus &lt;- tm_map(release_corpus, str_replace_all, pattern = "[[:punct:]]", replacement = " ")  
</code></pre>

<p>Remove stopwords</p>

<pre><code class="language-r">length(stopwords("en"))  
</code></pre>

<pre><code>## [1] 174
</code></pre>

<pre><code class="language-r">stopwords("en")[1:10]  
</code></pre>

<pre><code>##  [1] "i"         "me"        "my"        "myself"    "we"       
##  [6] "our"       "ours"      "ourselves" "you"       "your"
</code></pre>

<pre><code class="language-r">release_corpus &lt;- tm_map(release_corpus, removeWords, words = stopwords("en"))  
</code></pre>

<p>Convert to lower case</p>

<pre><code class="language-r">release_corpus &lt;- tm_map(release_corpus, tolower)  
</code></pre>

<p>Stem documents</p>

<pre><code class="language-r">release_corpus &lt;- tm_map(release_corpus, stemDocument)  
</code></pre>

<h6 id="1024sparsityandngrams">10.2.4 Sparsity and n-grams</h6>

<p>Create TermDocumentMatrix</p>

<pre><code class="language-r">tdm &lt;- TermDocumentMatrix(release_corpus)  
tdm  
</code></pre>

<pre><code>## A term-document matrix (8974 terms, 545 documents)
## 
## Non-/sparse entries: 82437/4808393
## Sparsity           : 98%
## Maximal term length: 34 
## Weighting          : term frequency (tf)
</code></pre>

<p>Remove sparse terms</p>

<pre><code class="language-r">tdm &lt;- removeSparseTerms(tdm, 1-(10/length(release_corpus)))  
tdm  
</code></pre>

<pre><code>## A term-document matrix (1587 terms, 545 documents)
## 
## Non-/sparse entries: 65410/799505
## Sparsity           : 92%
## Maximal term length: 22 
## Weighting          : term frequency (tf)
</code></pre>

<p>Create Bigrams</p>

<p>Fix - <a href="http://stackoverflow.com/a/20251039">http://stackoverflow.com/a/20251039</a></p>

<pre><code class="language-r"># Sets the default number of threads to use
options(mc.cores=1)  
</code></pre>

<pre><code class="language-r">BigramTokenizer &lt;- function(x){  
    NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
tdm_bigram &lt;- TermDocumentMatrix(release_corpus, control = list(tokenize = BigramTokenizer))  
tdm_bigram  
</code></pre>

<pre><code>## A term-document matrix (95648 terms, 545 documents)
## 
## Non-/sparse entries: 133515/51994645
## Sparsity           : 100%
## Maximal term length: 39 
## Weighting          : term frequency (tf)
</code></pre>

<p>Find associations</p>

<pre><code class="language-r">findAssocs(tdm, "nuclear", .7)  
</code></pre>

<pre><code>##        nuclear
## weapon    0.74
## treati    0.71
</code></pre>

<h6 id="103supervisedlearningtechniques">10.3 Supervised Learning Techniques</h6>

<h6 id="1035applicationgovernemntpressreleases">10.3.5 Application: Governemnt press releases</h6>

<pre><code class="language-r">dtm &lt;- DocumentTermMatrix(release_corpus)  
dtm &lt;- removeSparseTerms(dtm, 1-(10/length(release_corpus)))  
dtm  
</code></pre>

<pre><code>## A document-term matrix (545 documents, 1587 terms)
## 
## Non-/sparse entries: 65410/799505
## Sparsity           : 92%
## Maximal term length: 22 
## Weighting          : term frequency (tf)
</code></pre>

<p>Labels</p>

<pre><code class="language-r">org_labels &lt;- unlist(prescindMeta(release_corpus, "organisation")[,2])  
org_labels[1:3]  
</code></pre>

<pre><code>## [1] "Foreign &amp; Commonwealth Office"               
## [2] "Foreign &amp; Commonwealth Office"               
## [3] "Department for Business, Innovation &amp; Skills"
</code></pre>

<p>Create container</p>

<pre><code class="language-r">N &lt;- length(org_labels)  
container &lt;- create_container(  
    dtm,
    labels = org_labels,
    trainSize = 1:250,
    testSize = 251:N,
    virgin = F
)
slotNames(container)  
</code></pre>

<pre><code>## [1] "training_matrix"       "classification_matrix" "training_codes"       
## [4] "testing_codes"         "column_names"          "virgin"
</code></pre>

<p>Train models</p>

<pre><code class="language-r">svm_model &lt;- train_model(container, "SVM")  
tree_model &lt;- train_model(container, "TREE")  
#maxent_model &lt;- train_model(container, "MAXENT")
</code></pre>

<p>Models</p>

<pre><code class="language-r">svm_model  
</code></pre>

<pre><code>## 
## Call:
## svm.default(x = container@training_matrix, y = container@training_codes, 
##     kernel = kernel, cost = cost, cross = cross, probability = TRUE, 
##     method = method)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  100 
##       gamma:  0.0006301197 
## 
## Number of Support Vectors:  243
</code></pre>

<pre><code class="language-r">tree_model  
</code></pre>

<pre><code>## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 250 1156.000 Foreign &amp; Commonwealth Office ( 0.052000 0.108000 0.020000 0.020000 0.004000 0.040000 0.020000 0.040000 0.032000 0.008000 0.004000 0.256000 0.024000 0.008000 0.212000 0.092000 0.008000 0.004000 0.048000 )  
##     2) X952 &lt; 0.5 171  602.600 Foreign &amp; Commonwealth Office ( 0.005848 0.046784 0.005848 0.005848 0.000000 0.040936 0.005848 0.000000 0.035088 0.005848 0.005848 0.362573 0.000000 0.000000 0.304094 0.093567 0.011696 0.000000 0.070175 )  
##       4) X981 &lt; 0.5 120  430.100 Foreign &amp; Commonwealth Office ( 0.008333 0.066667 0.008333 0.008333 0.000000 0.058333 0.008333 0.000000 0.033333 0.008333 0.000000 0.466667 0.000000 0.000000 0.091667 0.125000 0.016667 0.000000 0.100000 )  
##         8) X1047 &lt; 2 107  359.600 Foreign &amp; Commonwealth Office ( 0.009346 0.074766 0.009346 0.009346 0.000000 0.065421 0.009346 0.000000 0.037383 0.009346 0.000000 0.523364 0.000000 0.000000 0.102804 0.018692 0.018692 0.000000 0.112150 )  
##          16) X1543 &lt; 0.5 97  304.000 Foreign &amp; Commonwealth Office ( 0.010309 0.082474 0.010309 0.010309 0.000000 0.072165 0.010309 0.000000 0.041237 0.010309 0.000000 0.577320 0.000000 0.000000 0.113402 0.020619 0.020619 0.000000 0.020619 )  
##            32) X586 &lt; 0.5 66  257.600 Foreign &amp; Commonwealth Office ( 0.015152 0.121212 0.015152 0.015152 0.000000 0.106061 0.015152 0.000000 0.060606 0.015152 0.000000 0.393939 0.000000 0.000000 0.151515 0.030303 0.030303 0.000000 0.030303 )  
##              64) X1012 &lt; 0.5 40  137.800 Foreign &amp; Commonwealth Office ( 0.000000 0.200000 0.025000 0.025000 0.000000 0.025000 0.025000 0.000000 0.075000 0.025000 0.000000 0.475000 0.000000 0.000000 0.050000 0.025000 0.000000 0.000000 0.050000 )  
##               128) X862 &lt; 0.5 28   79.180 Foreign &amp; Commonwealth Office ( 0.000000 0.071429 0.000000 0.000000 0.000000 0.035714 0.035714 0.000000 0.071429 0.000000 0.000000 0.607143 0.000000 0.000000 0.071429 0.035714 0.000000 0.000000 0.071429 )  
##                 256) X1439 &lt; 0.5 19   30.650 Foreign &amp; Commonwealth Office ( 0.000000 0.052632 0.000000 0.000000 0.000000 0.052632 0.000000 0.000000 0.052632 0.000000 0.000000 0.789474 0.000000 0.000000 0.000000 0.052632 0.000000 0.000000 0.000000 ) *
##                 257) X1439 &gt; 0.5 9   31.230 Ministry of Defence ( 0.000000 0.111111 0.000000 0.000000 0.000000 0.000000 0.111111 0.000000 0.111111 0.000000 0.000000 0.222222 0.000000 0.000000 0.222222 0.000000 0.000000 0.000000 0.222222 ) *
##               129) X862 &gt; 0.5 12   35.360 Department for Business, Innovation &amp; Skills ( 0.000000 0.500000 0.083333 0.083333 0.000000 0.000000 0.000000 0.000000 0.083333 0.083333 0.000000 0.166667 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
##              65) X1012 &gt; 0.5 26   84.630 Ministry of Defence ( 0.038462 0.000000 0.000000 0.000000 0.000000 0.230769 0.000000 0.000000 0.038462 0.000000 0.000000 0.269231 0.000000 0.000000 0.307692 0.038462 0.076923 0.000000 0.000000 )  
##               130) X1112 &lt; 0.5 19   44.780 Ministry of Defence ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.157895 0.000000 0.000000 0.000000 0.000000 0.000000 0.368421 0.000000 0.000000 0.421053 0.000000 0.052632 0.000000 0.000000 ) *
##               131) X1112 &gt; 0.5 7   20.650 Department for Environment, Food &amp; Rural Affairs ( 0.142857 0.000000 0.000000 0.000000 0.000000 0.428571 0.000000 0.000000 0.142857 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.142857 0.142857 0.000000 0.000000 ) *
##            33) X586 &gt; 0.5 31    8.835 Foreign &amp; Commonwealth Office ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.967742 0.000000 0.000000 0.032258 0.000000 0.000000 0.000000 0.000000 ) *
##          17) X1543 &gt; 0.5 10    0.000 Wales Office ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 ) *
##         9) X1047 &gt; 2 13    0.000 Prime Minister's Office, 10 Downing Street ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 0.000000 0.000000 0.000000 ) *
##       5) X981 &gt; 0.5 51   72.260 Ministry of Defence ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.039216 0.000000 0.019608 0.117647 0.000000 0.000000 0.803922 0.019608 0.000000 0.000000 0.000000 )  
##        10) X1234 &lt; 0.5 6   14.910 Foreign &amp; Commonwealth Office ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.166667 0.000000 0.166667 0.500000 0.000000 0.000000 0.166667 0.000000 0.000000 0.000000 0.000000 ) *
##        11) X1234 &gt; 0.5 45   40.900 Ministry of Defence ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.022222 0.000000 0.000000 0.066667 0.000000 0.000000 0.888889 0.022222 0.000000 0.000000 0.000000 ) *
##     3) X952 &gt; 0.5 79  375.900 Department for Business, Innovation &amp; Skills ( 0.151899 0.240506 0.050633 0.050633 0.012658 0.037975 0.050633 0.126582 0.025316 0.012658 0.000000 0.025316 0.075949 0.025316 0.012658 0.088608 0.000000 0.012658 0.000000 )  
##       6) X1071 &lt; 0.5 35  146.800 Department for Work and Pensions ( 0.114286 0.028571 0.057143 0.000000 0.028571 0.085714 0.000000 0.285714 0.057143 0.000000 0.000000 0.057143 0.000000 0.057143 0.028571 0.200000 0.000000 0.000000 0.000000 )  
##        12) X441 &lt; 0.5 25  104.900 Prime Minister's Office, 10 Downing Street ( 0.160000 0.040000 0.080000 0.000000 0.040000 0.120000 0.000000 0.000000 0.080000 0.000000 0.000000 0.080000 0.000000 0.080000 0.040000 0.280000 0.000000 0.000000 0.000000 )  
##          24) X1234 &lt; 0.5 8    6.028 Prime Minister's Office, 10 Downing Street ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.125000 0.000000 0.000000 0.000000 0.875000 0.000000 0.000000 0.000000 ) *
##          25) X1234 &gt; 0.5 17   70.330 Cabinet Office ( 0.235294 0.058824 0.117647 0.000000 0.058824 0.176471 0.000000 0.000000 0.117647 0.000000 0.000000 0.058824 0.000000 0.117647 0.058824 0.000000 0.000000 0.000000 0.000000 )  
##            50) X131 &lt; 0.5 11   36.120 Cabinet Office ( 0.363636 0.090909 0.000000 0.000000 0.090909 0.000000 0.000000 0.000000 0.181818 0.000000 0.000000 0.000000 0.000000 0.181818 0.090909 0.000000 0.000000 0.000000 0.000000 )  
##             100) X337 &lt; 0.5 6   15.960 Department of Energy &amp; Climate Change ( 0.000000 0.166667 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.333333 0.000000 0.000000 0.000000 0.000000 0.333333 0.166667 0.000000 0.000000 0.000000 0.000000 ) *
##             101) X337 &gt; 0.5 5    5.004 Cabinet Office ( 0.800000 0.000000 0.000000 0.000000 0.200000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
##            51) X131 &gt; 0.5 6   12.140 Department for Environment, Food &amp; Rural Affairs ( 0.000000 0.000000 0.333333 0.000000 0.000000 0.500000 0.000000 0.000000 0.000000 0.000000 0.000000 0.166667 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
##        13) X441 &gt; 0.5 10    0.000 Department for Work and Pensions ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
##       7) X1071 &gt; 0.5 44  149.200 Department for Business, Innovation &amp; Skills ( 0.181818 0.409091 0.045455 0.090909 0.000000 0.000000 0.090909 0.000000 0.000000 0.022727 0.000000 0.000000 0.136364 0.000000 0.000000 0.000000 0.000000 0.022727 0.000000 )  
##        14) X167 &lt; 1 26   89.700 Cabinet Office ( 0.307692 0.000000 0.076923 0.153846 0.000000 0.000000 0.153846 0.000000 0.000000 0.038462 0.000000 0.000000 0.230769 0.000000 0.000000 0.000000 0.000000 0.038462 0.000000 )  
##          28) X452 &lt; 0.5 14   29.530 Cabinet Office ( 0.571429 0.000000 0.000000 0.000000 0.000000 0.000000 0.285714 0.000000 0.000000 0.071429 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.071429 0.000000 )  
##            56) X194 &lt; 1.5 6   10.410 Department for Transport ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.666667 0.000000 0.000000 0.166667 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.166667 0.000000 ) *
##            57) X194 &gt; 1.5 8    0.000 Cabinet Office ( 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
##          29) X452 &gt; 0.5 12   24.270 HM Treasury ( 0.000000 0.000000 0.166667 0.333333 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.500000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 )  
##            58) X23 &lt; 1 6    7.638 Department for Culture, Media &amp; Sport ( 0.000000 0.000000 0.333333 0.666667 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
##            59) X23 &gt; 1 6    0.000 HM Treasury ( 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
##        15) X167 &gt; 1 18    0.000 Department for Business, Innovation &amp; Skills ( 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ) *
</code></pre>

<p>Classify models</p>

<pre><code class="language-r">svm_out &lt;- classify_model(container, svm_model)  
tree_out &lt;- classify_model(container, tree_model)  
#maxent_out &lt;- classify_model(container, maxent_model)
</code></pre>

<pre><code class="language-r">head(svm_out)  
</code></pre>

<pre><code>##                                      SVM_LABEL  SVM_PROB
## 1   Prime Minister's Office, 10 Downing Street 0.5857502
## 2                Foreign &amp; Commonwealth Office 0.3743644
## 3                Foreign &amp; Commonwealth Office 0.8146157
## 4 Department for Business, Innovation &amp; Skills 0.2582674
## 5                          Ministry of Defence 0.8829338
## 6                          Ministry of Defence 0.7160151
</code></pre>

<pre><code class="language-r">head(tree_out)  
</code></pre>

<pre><code>##                                   TREE_LABEL TREE_PROB
## 1 Prime Minister's Office, 10 Downing Street 1.0000000
## 2              Foreign &amp; Commonwealth Office 0.7894737
## 3              Foreign &amp; Commonwealth Office 0.5000000
## 4              Foreign &amp; Commonwealth Office 0.7894737
## 5                        Ministry of Defence 0.8888889
## 6                        Ministry of Defence 0.8888889
</code></pre>

<pre><code class="language-r">#head(maxent_out)
</code></pre>

<p>Construct data frame with correct labels</p>

<pre><code class="language-r">labels_out &lt;- data.frame(  
    correct_label = org_labels[251:N],
    svm = as.character(svm_out[,1]),
    tree = as.character(tree_out[,1]),
    #maxent = as.character(maxent_out[,1]),
    stringsAsFactors = F
)
</code></pre>

<p>SVM performance</p>

<pre><code class="language-r">table(labels_out[,1] == labels_out[,2])  
</code></pre>

<pre><code>## 
## FALSE  TRUE 
##    99   196
</code></pre>

<p>Random forest performance</p>

<pre><code class="language-r">table(labels_out[,1] == labels_out[,3])  
</code></pre>

<pre><code>## 
## FALSE  TRUE 
##   126   169
</code></pre>

<p>Maximum entropy performance</p>

<pre><code class="language-r">#table(labels_out[,1] == labels_out[,4])
</code></pre>

<pre><code class="language-r">prop.table(table(labels_out[,1] == labels_out[,3]))  
</code></pre>

<pre><code>## 
##     FALSE      TRUE 
## 0.4271186 0.5728814
</code></pre>

<h5 id="104unsupervisedlearningtechniques">10.4 Unsupervised Learning Techniques</h5>

<h5 id="1042applicationgovernmentpressreleases">10.4.2 Application: Government press releases</h5>

<p>Hierarchical clustering <br>
Create shortened corpus</p>

<pre><code class="language-r">short_corpus &lt;- release_corpus[c(  
    which(tm_index(
        release_corpus,
        FUN = sFilter,
        s = "organisation == 'Ministry of Defence'"))[1:20],
    which(tm_index(
        release_corpus,
        FUN = sFilter,
        s = "organisation == 'Wales Office'"))[1:20],
    which(tm_index(
        release_corpus,
        FUN = sFilter,
        s = "organisation == 'Department for Environment, Food &amp; Rural Affairs'"))[1:20]
)]
short_corpus  
</code></pre>

<pre><code>## A corpus with 60 text documents
</code></pre>

<pre><code class="language-r">table(as.character(prescindMeta(short_corpus, "organisation")[,2]))  
</code></pre>

<pre><code>## 
## Department for Environment, Food &amp; Rural Affairs 
##                                               20 
##                              Ministry of Defence 
##                                               20 
##                                     Wales Office 
##                                               20
</code></pre>

<p>Create shortened Document-Term-Matrix</p>

<pre><code class="language-r">short_dtm &lt;- DocumentTermMatrix(short_corpus)  
short_dtm &lt;- removeSparseTerms(short_dtm, 1-(5/length(short_corpus)))  
rownames(short_dtm) &lt;- c(rep("Defence", 20), rep("Wales", 20), rep("Environment", 20))  
</code></pre>

<p>Create dendrogram</p>

<pre><code class="language-r">dist_dtm &lt;- dist(short_dtm)  
out &lt;- hclust(dist_dtm, method = "ward")  
</code></pre>

<pre><code>## The "ward" method has been renamed to "ward.D"; note new "ward.D2"
</code></pre>

<pre><code class="language-r">plot(out)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-46-1.png" alt=""></p>

<p>Unsupervised classification</p>

<pre><code class="language-r">lda_out &lt;- LDA(dtm, 6)  
posterior_lda &lt;- posterior(lda_out)  
lda_topics &lt;- data.frame(t(posterior_lda$topics))  
</code></pre>

<p>Setting up matrix for mean probabilities</p>

<pre><code class="language-r">mean_topic_matrix &lt;- matrix(  
    NA,
    nrow = 6,
    ncol = 6,
    dimnames = list(
        names(table(org_labels)),
        str_c("Topic_", 1:6)
    )
)
## Filling matrix
for(i in 1:6){  
    mean_topic_matrix[i,] apply(lda_topics[, which(org_labels == rownames(mean_topic_matrix)[i])], 1, mean)
}
## Outputting rounded matrix
round(mean_topic_matrix, 2)

# Inspecting associated terms
terms(lda_out, 10)

# Correlated topic model
ctm_out &lt;- CTM(dtm, 6)  
terms(ctm_out, 10)

# Plotting the output
posterior_ctm &lt;- posterior(ctm_out)  
ctm_topics &lt;- data.frame(t(posterior_ctm$topics))

par(mfrow = c(2,3), cex.main = .8, pty = "s", mar = c(5, 5, 1, 1))  
for(topic in 1:2){  
    for(orga in names(table(org&lt;_labels))){
        tmp.data &lt;- ctm_topics[topic, org_labels == orga]
        plot(
            1:ncol(tmp.data),
            sort(as.numeric(tmp.data)),
            type = "l",
            ylim = c(0, 1),
            xlab = "Press releases",
            ylab = str_c("Posterior probability, topic ", topic),
            main = str_replace(orga, "Department for", "")
        )
    }
}
</code></pre>]]></description><link>http://localhost:2368/statistical-text-processing/</link><guid isPermaLink="false">70d39564-d404-46ed-99e0-9b7758c62b87</guid><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Thu, 20 Nov 2014 08:06:44 GMT</pubDate></item><item><title><![CDATA[Social Network analysis]]></title><description><![CDATA[<h1 id="socialnetworkanalysis">Social Network Analysis</h1>

<p>Social Network Analysis</p>

<p>Load the required packages. </p>

<pre><code class="language-r">##----------------------------- Social Network Analysis ------------------------------
if (!require(network)) {install.packages("network"); require(network)}  ## basic SNA stuff, relational data  
if (!require(igraph)) {install.packages("igraph"); require(igraph)}  ## fancy SNA stuff, main package  
if (!require(tm)) {install.packages("tm"); require(tm)}  ## text mining package  
</code></pre>

<h6 id="loadthedata">Load the data</h6>

<pre><code class="language-r">##------------------------------ SNA Data Structure -------------------------------
dat3490 &lt;- read.csv("http://people.fas.harvard.edu/~mair/psych3490/3490list.csv")  
</code></pre>

<p>First Name: In some cases this contains full name. </p>

<pre><code class="language-r">fmName &lt;- as.character(dat3490[,2])  
fmName                        ## first and middle names  
</code></pre>

<pre><code>##  [1] "Nicolas"           "Arthur"            "Donal Patrick"    
##  [4] "Aleksandr Garrett" "Chen"              "Erin"             
##  [7] "John David Nadal"  "Anna"              "David"            
## [10] "Yunfei"            "Aaron Edward"      "Andrew Garrett"   
## [13] "Jonathan"
</code></pre>

<p>Extract the first name only</p>

<pre><code class="language-r">fmsplit &lt;- strsplit(fmName, split = " ")     ## separate first and middle name in structure  
fmsplit  
</code></pre>

<pre><code>## [[1]]
## [1] "Nicolas"
## 
## [[2]]
## [1] "Arthur"
## 
## [[3]]
## [1] "Donal"   "Patrick"
## 
## [[4]]
## [1] "Aleksandr" "Garrett"  
## 
## [[5]]
## [1] "Chen"
## 
## [[6]]
## [1] "Erin"
## 
## [[7]]
## [1] "John"  "David" "Nadal"
## 
## [[8]]
## [1] "Anna"
## 
## [[9]]
## [1] "David"
## 
## [[10]]
## [1] "Yunfei"
## 
## [[11]]
## [1] "Aaron"  "Edward"
## 
## [[12]]
## [1] "Andrew"  "Garrett"
## 
## [[13]]
## [1] "Jonathan"
</code></pre>

<h5 id="herewego">Here we go</h5>

<pre><code class="language-r">fName &lt;- sapply(fmsplit, function(xx) xx[1]) ## extract first name only and store in vector  
fName  
</code></pre>

<pre><code>##  [1] "Nicolas"   "Arthur"    "Donal"     "Aleksandr" "Chen"     
##  [6] "Erin"      "John"      "Anna"      "David"     "Yunfei"   
## [11] "Aaron"     "Andrew"    "Jonathan"
</code></pre>

<pre><code class="language-r">N &lt;- length(fName)      ## number of participants  
</code></pre>

<h2 id="unidirectednetowrk">Unidirected Netowrk</h2>

<h5 id="helperfunctions">Helper Functions</h5>

<p>Diagonal Matrix  </p>

<pre><code class="language-r">diag(5)  
</code></pre>

<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    0    0    0    0
## [2,]    0    1    0    0    0
## [3,]    0    0    1    0    0
## [4,]    0    0    0    1    0
## [5,]    0    0    0    0    1
</code></pre>

<p>Access the upper Triangle of a Matrix  </p>

<pre><code class="language-r">upper.tri(diag(5))  
</code></pre>

<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] FALSE  TRUE  TRUE  TRUE  TRUE
## [2,] FALSE FALSE  TRUE  TRUE  TRUE
## [3,] FALSE FALSE FALSE  TRUE  TRUE
## [4,] FALSE FALSE FALSE FALSE  TRUE
## [5,] FALSE FALSE FALSE FALSE FALSE
</code></pre>

<p>Access lower triangle by tranposing upper triangle  </p>

<pre><code class="language-r">t(upper.tri(diag(5)))  
</code></pre>

<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] FALSE FALSE FALSE FALSE FALSE
## [2,]  TRUE FALSE FALSE FALSE FALSE
## [3,]  TRUE  TRUE FALSE FALSE FALSE
## [4,]  TRUE  TRUE  TRUE FALSE FALSE
## [5,]  TRUE  TRUE  TRUE  TRUE FALSE
</code></pre>

<h4 id="binaryneworkwithnoweightsundirectednetwork">Binary Nework with no weights, undirected network</h4>

<pre><code class="language-r">## --- binary (no weights), undirected network
set.seed(123)  
vec01 &lt;- sample(0:1, N*(N-1)/2, replace = TRUE)  
X &lt;- diag(N)  
X[upper.tri(X)] &lt;- vec01  
X &lt;- X + t(X)  
diag(X) &lt;- 0  
rownames(X) &lt;- colnames(X) &lt;- fName  
X                   ## binary, symmetric adjacency matrix  
</code></pre>

<pre><code>##           Nicolas Arthur Donal Aleksandr Chen Erin John Anna David Yunfei Aaron Andrew Jonathan
## Nicolas         0      0     1         1    1    1    1    1     0      1     0      0        1
## Arthur          0      0     0         1    1    0    0    1     0      0     0      0        1
## Donal           1      0     0         0    1    1    0    1     1      0     0      1        1
## Aleksandr       1      1     0         0    0    1    0    1     1      0     0      1        0
## Chen            1      1     1         0    0    0    1    1     1      0     1      0        1
## Erin            1      0     1         1    0    0    1    1     1      0     0      1        1
## John            1      0     0         0    1    1    0    1     0      0     0      0        1
## Anna            1      1     1         1    1    1    1    0     0      0     1      0        0
## David           0      0     1         1    1    1    0    0     0      0     0      0        0
## Yunfei          1      0     0         0    0    0    0    0     0      0     1      1        0
## Aaron           0      0     0         0    1    0    0    1     0      1     0      0        0
## Andrew          0      0     1         1    0    1    0    0     0      1     0      0        1
## Jonathan        1      1     1         0    1    1    1    0     0      0     0      1        0
</code></pre>

<p>Check for summetry of the resulting matrix  </p>

<pre><code class="language-r">isSymmetric(X)  
</code></pre>

<pre><code>## [1] TRUE
</code></pre>

<p><code>graph.adjacency</code>: function to create <code>igraph</code> from adjacency matrix.</p>

<pre><code class="language-r">net1 &lt;- graph.adjacency(X, mode = "undirected")      ## getting the data in shape in order to use it in igraph  
</code></pre>

<p>Plot the network</p>

<pre><code class="language-r">plot(net1)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-12-1.png" alt=""></p>

<p>Get the edges</p>

<pre><code class="language-r">el1 &lt;- get.edgelist(net1, names = TRUE)    ## this would be the corresponding edge list  
el1[1:10]  
</code></pre>

<pre><code>##  [1] "Nicolas" "Nicolas" "Nicolas" "Nicolas" "Nicolas" "Nicolas" "Nicolas"
##  [8] "Nicolas" "Arthur"  "Arthur"
</code></pre>

<h2 id="weighteddirectednetwork">Weighted, directed network</h2>

<pre><code class="language-r">##--- weighted, directed network
set.seed(123)  
veck &lt;- rpois(N*N, lambda = 0.5)   ## create weights (drawing from a Poisson distribution)  
veck  
</code></pre>

<pre><code>##   [1] 0 1 0 1 2 0 0 1 0 0 2 0 1 0 0 1 0 0 0 2 1 1 1 3 1 1 0 0 0 0 2 1 1 1 0
##  [36] 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0
##  [71] 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 2 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 2 0
## [106] 1 2 1 0 0 2 0 0 2 1 0 0 2 0 0 1 0 0 0 0 2 0 0 0 1 1 1 1 1 0 1 1 1 2 0
## [141] 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0
</code></pre>

<pre><code class="language-r">hist(veck)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-15-1.png" alt=""></p>

<p>Create some outliers</p>

<pre><code class="language-r">veck[c(4, 20)] &lt;- 8                ## let's create some outliers  
hist(veck)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-16-1.png" alt=""></p>

<p>Populate the matrix  </p>

<pre><code class="language-r">X &lt;- matrix(veck, ncol = N)  
diag(X) &lt;- 0  
rownames(X) &lt;- colnames(X) &lt;- fName  
X        ## adjacency matrix (not symmetric, weights)  
</code></pre>

<pre><code>##           Nicolas Arthur Donal Aleksandr Chen Erin John Anna David Yunfei Aaron Andrew Jonathan
## Nicolas         0      0     0         0    1    0    0    1     0      2     1      0        0
## Arthur          1      0     0         0    0    1    0    0     1      0     1      1        0
## Donal           0      1     0         0    0    1    0    1     2      0     1      0        0
## Aleksandr       8      0     0         0    0    1    1    0     1      1     1      0        0
## Chen            2      0     2         0    0    0    0    0     0      0     0      0        0
## Erin            0      0     1         0    1    0    1    1     0      0     1      0        0
## John            0      8     1         0    1    1    0    0     2      0     1      1        1
## Anna            1      1     1         0    0    1    0    0     0      0     1      1        0
## David           0      1     0         0    1    0    2    0     0      2     2      0        0
## Yunfei          0      1     0         0    0    0    1    0     2      0     0      0        0
## Aaron           2      3     1         1    0    0    1    0     1      0     0      0        1
## Andrew          0      1     0         0    0    0    0    0     0      0     0      0        0
## Jonathan        1      1     0         0    1    1    0    2     0      1     0      0        0
</code></pre>

<p>Resulting Matrix is not symmetric  </p>

<pre><code class="language-r">isSymmetric(X)  
</code></pre>

<pre><code>## [1] FALSE
</code></pre>

<p>Getting the data in shape.  </p>

<pre><code class="language-r">net2 &lt;- graph.adjacency(X, weighted = TRUE)      ## getting the data in shape in order to use it in igraph  
</code></pre>

<p>Get the edges of this network  </p>

<pre><code class="language-r">E(net2)  
</code></pre>

<pre><code>## Edge sequence:
##                            
## [1]  Nicolas   -&gt; Chen     
## [2]  Nicolas   -&gt; Anna     
## [3]  Nicolas   -&gt; Yunfei   
## [4]  Nicolas   -&gt; Aaron    
## [5]  Arthur    -&gt; Nicolas  
## [6]  Arthur    -&gt; Erin     
## [7]  Arthur    -&gt; David     
...
## [57] Andrew    -&gt; Arthur   
## [58] Jonathan  -&gt; Nicolas  
## [59] Jonathan  -&gt; Arthur   
## [60] Jonathan  -&gt; Chen     
## [61] Jonathan  -&gt; Erin     
## [62] Jonathan  -&gt; Anna     
## [63] Jonathan  -&gt; Yunfei
</code></pre>

<p>What is the weight of each edge</p>

<pre><code class="language-r">E(net2)$weight                  ## edge weights  
</code></pre>

<pre><code>##  [1] 1 1 2 1 1 1 1 1 1 1 1 1 2 1 8 1 1 1 1 1 2 2 1 1 1 1 1 8 1 1 1 2 1 1 1
## [36] 1 1 1 1 1 1 1 1 2 2 2 1 1 2 2 3 1 1 1 1 1 1 1 1 1 1 2 1
</code></pre>

<p>Vertices</p>

<pre><code class="language-r">V(net2)  
</code></pre>

<pre><code>## Vertex sequence:
##  [1] "Nicolas"   "Arthur"    "Donal"     "Aleksandr" "Chen"     
##  [6] "Erin"      "John"      "Anna"      "David"     "Yunfei"   
## [11] "Aaron"     "Andrew"    "Jonathan"
</code></pre>

<pre><code class="language-r">plot(net2,  edge.arrow.size = 0.5, edge.width = E(net2)$weight,  
     layout = layout.fruchterman.reingold)
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-23-1.png" alt=""></p>

<p><code>plot.igraph</code> provides numerous options for customizing the plot; see help</p>

<pre><code class="language-r">?igraph.plotting
</code></pre>

<h4 id="interactiveplots">interactive plots</h4>

<pre><code class="language-r">tkplot(net2,  edge.arrow.size = 0.5, edge.width = E(net2)$weight)  
</code></pre>

<pre><code class="language-r">##------------------------------ Centrality and Prestige ------------------------
## ------- undirected networks ------
## Florentine families: During the renaissance (14th-17th century) Florence
## was the banking center of Europe (mostly due to the Medicis). During this period
## of time many aristocratic families lived in Florence. Aristocratic family
## members tend to marry members from other aristocratic families. 
## In this example we explore the Florentine marriage network and compute 
## various network measures

data(flo)  
flo                ## adjacency matrix, undirected --&gt; symmetric  
</code></pre>

<pre><code>##              Acciaiuoli Albizzi Barbadori Bischeri Castellani Ginori  Guadagni Lamberteschi Medici Pazzi Peruzzi Pucci Ridolfi Salviati Strozzi Tornabuoni
## Acciaiuoli            0       0         0        0          0      0         0            0      1     0       0     0       0        0       0          0
## Albizzi               0       0         0        0          0      1         1            0      1     0       0     0       0        0       0          0
## Barbadori             0       0         0        0          1      0         0            0      1     0       0     0       0        0       0          0
## Bischeri              0       0         0        0          0      0         1            0      0     0       1     0       0        0       1          0
## Castellani            0       0         1        0          0      0         0            0      0     0       1     0       0        0       1          0
## Ginori                0       1         0        0          0      0         0            0      0     0       0     0       0        0       0          0
## Guadagni              0       1         0        1          0      0         0            1      0     0       0     0       0        0       0          1
## Lamberteschi          0       0         0        0          0      0         1            0      0     0       0     0       0        0       0          0
## Medici                1       1         1        0          0      0         0            0      0     0       0     0       1        1       0          1
## Pazzi                 0       0         0        0          0      0         0            0      0     0       0     0       0        1       0          0
## Peruzzi               0       0         0        1          1      0         0            0      0     0       0     0       0        0       1          0
## Pucci                 0       0         0        0          0      0         0            0      0     0       0     0       0        0       0          0
## Ridolfi               0       0         0        0          0      0         0            0      1     0       0     0       0        0       1          1
## Salviati              0       0         0        0          0      0         0            0      1     1       0     0       0        0       0          0
## Strozzi               0       0         0        1          1      0         0            0      0     0       1     0       1        0       0          0
## Tornabuoni            0       0         0        0          0      0         1            0      1     0       0     0       1        0       0          0
</code></pre>

<pre><code class="language-r">isSymmetric(flo)  
</code></pre>

<pre><code>## [1] TRUE
</code></pre>

<pre><code class="language-r">floG &lt;- graph.adjacency(flo, mode = "undirected")   ## let's just convert the adjacency matrix into an igraph object

plot(floG, vertex.size = 0, edge.arrow.size = 0.25,  
     vertex.label.dist = 0.5)     
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-28-1.png" alt=""></p>

<p>let's remove the poor Pucci's</p>

<pre><code class="language-r">floG1 &lt;- delete.vertices(floG, v = "Pucci")  
plot(floG1, vertex.size = 0, edge.arrow.size = 0.25,  
     vertex.label.dist = 0.5)  
</code></pre>

<p><img src="http://localhost:2368/content/images/2014/11/unnamed-chunk-29-1.png" alt=""></p>

<p>edges</p>

<pre><code class="language-r">E(floG1)               ## edges  
</code></pre>

<pre><code>## Edge sequence:
##                                  
## [1]  Medici       -- Acciaiuoli  
## [2]  Ginori       -- Albizzi     
## [3]  Guadagni     -- Albizzi     
## [4]  Medici       -- Albizzi     
## [5]  Castellani   -- Barbadori   
## [6]  Medici       -- Barbadori   
## [7]  Guadagni     -- Bischeri    
## [8]  Peruzzi      -- Bischeri    
## [9]  Strozzi      -- Bischeri    
## [10] Peruzzi      -- Castellani  
## [11] Strozzi      -- Castellani  
## [12] Lamberteschi -- Guadagni    
## [13] Tornabuoni   -- Guadagni    
## [14] Ridolfi      -- Medici      
## [15] Salviati     -- Medici      
## [16] Tornabuoni   -- Medici      
## [17] Salviati     -- Pazzi       
## [18] Strozzi      -- Peruzzi     
## [19] Strozzi      -- Ridolfi     
## [20] Tornabuoni   -- Ridolfi
</code></pre>

<p>vertices</p>

<pre><code class="language-r">V(floG1)               ## vertices  
</code></pre>

<pre><code>## Vertex sequence:
##  [1] "Acciaiuoli"   "Albizzi"      "Barbadori"    "Bischeri"    
##  [5] "Castellani"   "Ginori"       "Guadagni"     "Lamberteschi"
##  [9] "Medici"       "Pazzi"        "Peruzzi"      "Ridolfi"     
## [13] "Salviati"     "Strozzi"      "Tornabuoni"
</code></pre>

<p>number of nodes</p>

<pre><code class="language-r">N &lt;- length(V(floG1))  ## number of nodes  
</code></pre>

<pre><code class="language-r">## -- Nodal degree:
degvec &lt;- sort(degree(floG1), decreasing = TRUE)  
degvec  
</code></pre>

<pre><code>##       Medici     Guadagni      Strozzi      Albizzi     Bischeri 
##            6            4            4            3            3 
##   Castellani      Peruzzi      Ridolfi   Tornabuoni    Barbadori 
##            3            3            3            3            2 
##     Salviati   Acciaiuoli       Ginori Lamberteschi        Pazzi 
##            2            1            1            1            1
</code></pre>

<pre><code class="language-r">mean(degvec)     ## useful if we want to compare different marriage networks  
</code></pre>

<pre><code>## [1] 2.666667
</code></pre>

<h5 id="density">Density:</h5>

<p>Density of the graph is number of edges to number of possible edges</p>

<pre><code class="language-r">graph.density(floG1)  ## ~19% of all possible edges are present  
</code></pre>

<pre><code>## [1] 0.1904762
</code></pre>

<h5 id="centralitymeasures">Centrality measures:</h5>

<h6 id="degreecentrality">Degree centrality</h6>

<pre><code class="language-r">degCvec &lt;- sort(degree(floG1), decreasing = TRUE)  ## degree centrality  
degCvec  
</code></pre>

<pre><code>##       Medici     Guadagni      Strozzi      Albizzi     Bischeri 
##            6            4            4            3            3 
##   Castellani      Peruzzi      Ridolfi   Tornabuoni    Barbadori 
##            3            3            3            3            2 
##     Salviati   Acciaiuoli       Ginori Lamberteschi        Pazzi 
##            2            1            1            1            1
</code></pre>

<pre><code class="language-r">## since the degree depends on the number of actors we can standardize 
## it by the number of nodes - 1 (e.g. for comparing across networks)

degCvec/(N-1)  
</code></pre>

<pre><code>##       Medici     Guadagni      Strozzi      Albizzi     Bischeri 
##   0.42857143   0.28571429   0.28571429   0.21428571   0.21428571 
##   Castellani      Peruzzi      Ridolfi   Tornabuoni    Barbadori 
##   0.21428571   0.21428571   0.21428571   0.21428571   0.14285714 
##     Salviati   Acciaiuoli       Ginori Lamberteschi        Pazzi 
##   0.14285714   0.07142857   0.07142857   0.07142857   0.07142857
</code></pre>

<pre><code class="language-r">centralization.degree(floG1)  ## group degree centralization  
</code></pre>

<pre><code>## $res
##  [1] 1 3 2 3 3 1 4 1 6 1 3 3 2 4 3
## 
## $centralization
## [1] 0.2380952
## 
## $theoretical_max
## [1] 210
</code></pre>

<h5 id="closenesscentrality">Closeness centrality</h5>

<pre><code class="language-r">sort(closeness(floG1), decreasing = TRUE)   ## closeness centrality  
</code></pre>

<pre><code>##       Medici      Ridolfi      Albizzi   Tornabuoni     Guadagni 
##   0.04000000   0.03571429   0.03448276   0.03448276   0.03333333 
##    Barbadori      Strozzi     Bischeri   Castellani     Salviati 
##   0.03125000   0.03125000   0.02857143   0.02777778   0.02777778 
##   Acciaiuoli      Peruzzi       Ginori Lamberteschi        Pazzi 
##   0.02631579   0.02631579   0.02380952   0.02325581   0.02040816
</code></pre>

<pre><code class="language-r">shortest.paths(floG1)     ## matrix with geodesics  
</code></pre>

<pre><code>##              Acciaiuoli Albizzi Barbadori Bischeri Castellani Ginori
## Acciaiuoli            0       2         2        4          3      3
## Albizzi               2       0         2        2          3      1
## Barbadori             2       2         0        3          1      3
## Bischeri              4       2         3        0          2      3
## Castellani            3       3         1        2          0      4
## Ginori                3       1         3        3          4      0
## Guadagni              3       1         3        1          3      2
## Lamberteschi          4       2         4        2          4      3
## Medici                1       1         1        3          2      2
## Pazzi                 3       3         3        5          4      4
## Peruzzi               4       3         2        1          1      4
## Ridolfi               2       2         2        2          2      3
## Salviati              2       2         2        4          3      3
## Strozzi               3       3         2        1          1      4
## Tornabuoni            2       2         2        2          3      3
##              Guadagni Lamberteschi Medici Pazzi Peruzzi Ridolfi Salviati
## Acciaiuoli          3            4      1     3       4       2        2
## Albizzi             1            2      1     3       3       2        2
## Barbadori           3            4      1     3       2       2        2
## Bischeri            1            2      3     5       1       2        4
## Castellani          3            4      2     4       1       2        3
## Ginori              2            3      2     4       4       3        3
## Guadagni            0            1      2     4       2       2        3
## Lamberteschi        1            0      3     5       3       3        4
## Medici              2            3      0     2       3       1        1
## Pazzi               4            5      2     0       5       3        1
## Peruzzi             2            3      3     5       0       2        4
## Ridolfi             2            3      1     3       2       0        2
## Salviati            3            4      1     1       4       2        0
## Strozzi             2            3      2     4       1       1        3
## Tornabuoni          1            2      1     3       3       1        2
##              Strozzi Tornabuoni
## Acciaiuoli         3          2
## Albizzi            3          2
## Barbadori          2          2
## Bischeri           1          2
## Castellani         1          3
## Ginori             4          3
## Guadagni           2          1
## Lamberteschi       3          2
## Medici             2          1
## Pazzi              4          3
## Peruzzi            1          3
## Ridolfi            1          1
## Salviati           3          2
## Strozzi            0          2
## Tornabuoni         2          0
</code></pre>

<pre><code class="language-r">centralization.closeness(floG1)  
</code></pre>

<pre><code>## $res
##  [1] 0.3684211 0.4827586 0.4375000 0.4000000 0.3888889 0.3333333 0.4666667
##  [8] 0.3255814 0.5600000 0.2857143 0.3684211 0.5000000 0.3888889 0.4375000
## [15] 0.4827586
## 
## $centralization
## [1] 0.3224523
## 
## $theoretical_max
## [1] 6.740741
</code></pre>

<pre><code class="language-r">## Betweenness centrality
sort(betweenness(floG1, directed = FALSE, normalized = TRUE), decreasing = TRUE)  ## node betweenness  
centralization.betweenness(floG1, directed = FALSE)  ## aggregate measure

ebet &lt;- edge.betweenness(floG1, directed = FALSE) ## edge betweenness  
ebet  
el &lt;- get.edgelist(floG1)  
el  
data.frame(from = el[,1], to = el[,2], betweenness = ebet)

## ----- directed networks -----
## Let's now look at some directed trade data between countries
if (!require(SNAData)) {install.packages("SNAData"); require(SNAData)}  ## datasets  
data(basicGoods)     ## it's a graphNEL object (general object structure for graphs) 

tradenet &lt;- igraph.from.graphNEL(basicGoods)  ## convert it to an igraph object

x11()  
plot(tradenet, vertex.size = 0, edge.arrow.size = 0.4,  
     vertex.label.dist = 0.5)

get.adjacency(tradenet)   ## adjacency matrix  
get.edgelist(tradenet)

## --- Indegree and Outdegree
## Degree Prestige (same as indegree)
sort(igraph:::degree(tradenet, mode = c("in")), decreasing = TRUE)  ## indegree (imports)

## Outdegree 
sort(igraph:::degree(tradenet, mode = c("out")), decreasing = TRUE) ## outdegree (exports)

## --- Centrality/Prestige
## Centrality as above (takes outgoing edges) 
sort(closeness(tradenet), decreasing = TRUE)   ## exports  
## Prestige (takes ingoing edges)
sort(closeness(tradenet, mode = "in"), decreasing = TRUE)    ## imports


##-------------------------- Cohesive Subgroups -------------------------------
load(url("http://people.fas.harvard.edu/~mair/psych3490/statementsGOP.rda"))  

inspect(gopCorp.unique)   ## 254 statements

inspect(tm_filter(gopCorp.unique, FUN = function(x) any(grep("sanctity", x))))   ## as an example

##--- re-organize the data as edge list
gopCorp.unique1 &lt;- tm_map(gopCorp.unique, tolower)  
mystopwords &lt;- c("beleive", "shld", "1", "-", "wenot", "conservatismthe", "etc", "im",  
                 "fatherthe", "conservativebelieve", "governmentprolife2nd",
                 "amendmentand", "valuessmall", "ive", "4", "familyrepublican",
                 "-government", "1st", "believe", "belive", "still", "dont", 
                 "want","seen", "b", "w","can")
statementsGOP &lt;- Corpus(VectorSource(lapply(gopCorp.unique1, removeWords,  
                                            c(mystopwords, stopwords("english")))))
slen &lt;- unlist(tm_map(statementsGOP, function(tf) sum(termFreq(tf))))  ## statement length  
statementsGOP &lt;- statementsGOP[which(slen &gt; 1)]                        ## select the statements with more than 1 word  
statementsGOP

## function to get pairwise word structure suited for directed network graph
textsna &lt;- function(str1) {  
  splitState &lt;- scan_tokenizer(str1)
  indmat &lt;- matrix(c(1, rep(2:length(splitState), each = 2)), ncol = 2, 
                   nrow = (length(splitState) - 1))
  matrix(splitState[indmat], ncol = 2, byrow = TRUE)
}  
statementPairs &lt;- do.call(rbind, lapply(statementsGOP, textsna))   ## warnings can be ignored

statementsGOP[[1]]  
statementPairs[1:11,]   ## in row 11 the 2nd statement starts  
## ----- end data preparation

## ------ create network and produce network plot
statementGraph &lt;- graph.edgelist(statementPairs, directed = TRUE) ## make a graph with each row entry as a from-to vertex and an edge  
E(statementGraph)$weight &lt;- count.multiple(statementGraph)        ## edge frequencies -- count how often an edge has exactly the same tail and edge vertices  
statementGraph &lt;- simplify(statementGraph, edge.attr.comb =  
                             list(weight = max, name = "concat", "ignore") ) ## keep multiplicity as edges
E(statementGraph)$weight

set.seed(123)  
ly &lt;- layout.fruchterman.reingold(statementGraph)    ## define layout (more concentric)  
## ly &lt;- layout.kamada.kawai(statementGraph)         ## more branches going out of the center


x11()  
plot(statementGraph, layout = ly, vertex.size = 2, edge.color = "lightgray",  
     edge.arrow.size = 0.05, edge.curved = FALSE, 
     vertex.label = NA, 
     rescale = FALSE, 
     xlim = c(-530, 630), ylim = c(-440, 600),
     main= "Republican Statements (Large Communities)", asp = 0,
     margin = -1)


##--------- compute cliques ------------
cliqList &lt;- cliques(statementGraph, min = 4)   ## find and all cliques (at least 4 nodes)  
cliqList                   ## returns the node index only  
V(statementGraph)$name           ## gives the names  
cliqlistN &lt;- lapply(cliqList, function(xx) {  
  V(statementGraph)$name[xx]
})
cliqlistN

## let's extract the largest clique and produce a plot
cliqLarge &lt;- largest.cliques(statementGraph)  
cliqLarge  
statementGraphSub &lt;- induced.subgraph(statementGraph, cliqLarge[[1]])  
x11()  
plot(statementGraphSub, vertex.size = 2, edge.color = "gray",  
     vertex.color= "black", vertex.label.color = "blue",
     edge.arrow.size = 1, edge.curved = FALSE, edge.width = E(statementGraphSub)$weight,
     vertex.label.dist = 0.5, vertex.label.cex = 0.8, vertex.label.font = 2)



## ------ compute communities ---------
## A nice blog entry that decribes various community approaches can be found here:
## browseURL("http://stackoverflow.com/questions/9471906/what-are-the-differences-between-community-detection-algorithms-in-igraph")

## We use the wakltrap algorthim
xc &lt;- walktrap.community(statementGraph, weights = E(statementGraph)$weight, step = 6)   ## very quick  
membership(xc)        ## gives the community membership for each node

edgeList &lt;- tapply(seq_along(membership(xc)), membership(xc), function(xx) xx) ## communities without labels  
edgeList                                ## communities as list (containing node index)  
comList &lt;- tapply(membership(xc), membership(xc), names)  
comList                                 ## communities as list (containing node names)  
length(comList)                         ## number of communities  
comsize &lt;- sapply(comList, length)  
table(comsize)  
barplot(table(comsize), main = "Community Size")   ## frequency distribution of community sizes

bigComIndex &lt;- which(comsize &gt; 20) ## big communities (having more than 20 nodes)  
bigComIndex  
comList[bigComIndex]

coms3 &lt;- edgeList[bigComIndex[[1]]]    ## pull out first community  
coms4 &lt;- edgeList[bigComIndex[[3]]]    ## pull out third community


## ------- produce a fancy plot: full network, big communities added
x11()  
vcolo &lt;- c(hcl(h=0,35,60),hcl(h=72,c=35,l=60),hcl(h=144,c=35,l=60),hcl(h=216,c=35,l=60),hcl(h=320,c=35,l=60))  
op &lt;- par(mar = c(1,2,1,1))  
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))  
plot(statementGraph, layout = ly, vertex.size = 2, edge.color = "lightgray",  
     edge.arrow.size = 0.05, edge.curved = FALSE, 
     vertex.label = NA, 
     rescale = FALSE, 
     xlim = c(-530, 630), ylim = c(-440, 600),
     main= "Republican Statements (Large Communities)", asp = 0,
     margin = -1)
comGraphBig1 &lt;- induced.subgraph(statementGraph, coms3[[1]])  
ly1 &lt;- ly[unlist(coms3[[1]]),]  
plot(comGraphBig1, layout = ly1, vertex.size = 3, edge.color = vcolo[2],  
     edge.arrow.size = 0.2, edge.curved = FALSE,  vertex.label = NA, 
     vertex.label.dist = 0, vertex.label.cex = 0.8, vertex.label.font = 2,
     rescale = FALSE, add=TRUE, vertex.color=vcolo[2],vertex.frame.color=vcolo[2],vertex.label.color=vcolo[2])
comGraphBig2 &lt;- induced.subgraph(statementGraph, coms4[[1]])  
ly1 &lt;- ly[unlist(coms4[[1]]),]  
plot(comGraphBig2, layout = ly1, vertex.size = 3, edge.color = vcolo[3],  
     edge.arrow.size = 0.2, edge.curved = FALSE,  vertex.label = NA, 
     vertex.label.dist = 0, vertex.label.cex = 0.8, vertex.label.font = 2,
     rescale = FALSE, add=TRUE, vertex.color=vcolo[3],vertex.frame.color=vcolo[3],vertex.label.color=vcolo[3])
op2 &lt;- par(mar = c(4,1,1,1))  
colo &lt;- c(hcl(72,35,40),hcl(72,35,80))  
comGraphBig1 &lt;- induced.subgraph(statementGraph, unlist(edgeList[bigComIndex[1]]))  
ly1 &lt;- ly[unlist(edgeList[bigComIndex[1]]),]  
plot(comGraphBig1, layout = ly1, vertex.size = 2, edge.color = colo[2],  
     vertex.color=colo[1],vertex.label.color=colo[1],
     edge.arrow.size = 0.20, edge.curved = FALSE, edge.width = E(comGraphBig1)$weight/2,
     vertex.label.dist = 0, vertex.label.cex = 0.8, vertex.label.font = 2,
     rescale = FALSE, xlim=range(ly1[,1]), ylim=range(ly1[,2]))
colo &lt;- c(hcl(144,35,40),hcl(144,35,80))  
comGraphBig2 &lt;- induced.subgraph(statementGraph, unlist(edgeList[bigComIndex[3]]))  
ly1 &lt;- ly[unlist(edgeList[bigComIndex[3]]),]  
plot(comGraphBig2, layout = ly1, vertex.size = 2, edge.color = colo[2],  
     vertex.color=colo[1],vertex.label.color=colo[1],
     edge.arrow.size = 0.20, edge.curved = FALSE, edge.width = E(comGraphBig2)$weight/2,
     vertex.label.dist = 0, vertex.label.cex = 0.8, vertex.label.font = 2,
     rescale = FALSE, xlim=range(ly1[,1]), ylim=range(ly1[,2]))
par(op)  
par(op2)  
</code></pre>]]></description><link>http://localhost:2368/social-network-analysis/</link><guid isPermaLink="false">b2fa6250-0cf2-4a2c-9f93-cb69c8064f76</guid><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Thu, 20 Nov 2014 01:16:52 GMT</pubDate></item><item><title><![CDATA[Parallelization in R]]></title><description><![CDATA[<p>Running R scripts in parallel</p>

<p>R <a href="https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">Parallel</a> package.</p>

<pre><code class="language-r">library(parallel)  
</code></pre>

<p>inputs and operations to be performed on each input</p>

<pre><code class="language-r">inputs  &lt;- 1:10  
processInput  &lt;- function(i) {  
  i * i
}
</code></pre>

<p>Find the number of cores</p>

<pre><code class="language-r">numCores  &lt;- detectCores()  
</code></pre>

<pre><code class="language-r">results = mclapply(inputs, processInput, mc.cores = numCores)  
</code></pre>

<p>For Windwos OS only  </p>

<pre><code class="language-r"># the above won't work on Windows, but this will:
cl &lt;- makeCluster(numCores)  
results = parLapply(cl, inputs, processInput)  
stopCluster(cl)  
</code></pre>

<p>Difference between <code>mclapply</code> and <code>parLapply</code> can be found on this <a href="http://stackoverflow.com/questions/17196261/understanding-the-differences-between-mclapply-and-parlapply-in-r">StackOverflow post</a></p>

<p>As an alternative, we can also use the <a href="http://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf">foreach package</a>, which lets us use a familiar <code>for</code> loop syntax, automatically parallelizing your code under the hood:</p>

<pre><code class="language-r">library(foreach)  
library(doParallel)  
library(parallel)

numCores &lt;- detectCores()  
cl &lt;- makeCluster(numCores)  
registerDoParallel(cl)

inputs &lt;- 1:10  
processInput &lt;- function(i) {  
  i * i
}

results &lt;- foreach(i=inputs) %dopar% {  
  processInput(i)
}
</code></pre>]]></description><link>http://localhost:2368/parallelization-in-r/</link><guid isPermaLink="false">b620174d-62a4-46f9-9d87-0f857bb102fe</guid><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Tue, 18 Nov 2014 02:01:21 GMT</pubDate></item><item><title><![CDATA[Welcome to Ghost]]></title><description><![CDATA[<p>You're live! Nice. We've put together a little post to introduce you to the Ghost editor and get you started. You can manage your content by signing in to the admin area at <code>&lt;your blog URL&gt;/ghost/</code>. When you arrive, you can select this post from a list on the left and see a preview of it on the right. Click the little pencil icon at the top of the preview to edit this post and read the next section!</p>

<h2 id="gettingstarted">Getting Started</h2>

<p>Ghost uses something called Markdown for writing. Essentially, it's a shorthand way to manage your post formatting as you write!</p>

<p>Writing in Markdown is really easy. In the left hand panel of Ghost, you simply write as you normally would. Where appropriate, you can use <em>shortcuts</em> to <strong>style</strong> your content. For example, a list:</p>

<ul>
<li>Item number one</li>
<li>Item number two
<ul><li>A nested item</li></ul></li>
<li>A final item</li>
</ul>

<p>or with numbers!</p>

<ol>
<li>Remember to buy some milk  </li>
<li>Drink the milk  </li>
<li>Tweet that I remembered to buy the milk, and drank it</li>
</ol>

<h3 id="links">Links</h3>

<p>Want to link to a source? No problem. If you paste in a URL, like <a href="http://ghost.org/">http://ghost.org</a> - it'll automatically be linked up. But if you want to customise your anchor text, you can do that too! Here's a link to <a href="http://ghost.org/">the Ghost website</a>. Neat.</p>

<h3 id="whataboutimages">What about Images?</h3>

<p>Images work too! Already know the URL of the image you want to include in your article? Simply paste it in like this to make it show up:</p>

<p><img src="https://ghost.org/images/ghost.png" alt="The Ghost Logo"></p>

<p>Not sure which image you want to use yet? That's ok too. Leave yourself a descriptive placeholder and keep writing. Come back later and drag and drop the image in to upload:</p>

<h3 id="quoting">Quoting</h3>

<p>Sometimes a link isn't enough, you want to quote someone on what they've said. It was probably very wisdomous. Is wisdomous a word? Find out in a future release when we introduce spellcheck! For now - it's definitely a word.</p>

<blockquote>
  <p>Wisdomous - it's definitely a word.</p>
</blockquote>

<h3 id="workingwithcode">Working with Code</h3>

<p>Got a streak of geek? We've got you covered there, too. You can write inline <code>&lt;code&gt;</code> blocks really easily with back ticks. Want to show off something more comprehensive? 4 spaces of indentation gets you there.</p>

<pre><code>.awesome-thing {
    display: block;
    width: 100%;
}
</code></pre>

<h3 id="readyforabreak">Ready for a Break?</h3>

<p>Throw 3 or more dashes down on any new line and you've got yourself a fancy new divider. Aw yeah.</p>

<hr>

<h3 id="advancedusage">Advanced Usage</h3>

<p>There's one fantastic secret about Markdown. If you want, you can write plain old HTML and it'll still work! Very flexible.</p>

<p><input type="text" placeholder="I'm an input field!"></p>

<p>That should be enough to get you started. Have fun - and let us know what you think :)</p>]]></description><link>http://localhost:2368/welcome-to-ghost-2/</link><guid isPermaLink="false">a8120aea-372c-489e-aed0-f687069ac0da</guid><category><![CDATA[Getting Started]]></category><dc:creator><![CDATA[Vikas Gupta]]></dc:creator><pubDate>Mon, 17 Nov 2014 03:21:18 GMT</pubDate></item></channel></rss>